{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2502457f-e57c-4789-b891-c358dae7b83b",
    "_uuid": "9c85c16e-4435-4725-a2ba-85e0c50a8dfc"
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5981560e-1173-46f2-a873-5123d72ee336",
    "_uuid": "a4b37630-0bae-426c-a29c-478bd65b99f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_2.pkl\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_2.pkl\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/__results__.html\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_1.pkl\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_1.pkl\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_3.pkl\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/__notebook__.ipynb\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/custom.css\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/__output__.json\n",
      "/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_3.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/__results__.html\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_2.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/__notebook__.ipynb\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/custom.css\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_1.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-1-2/__output__.json\n",
      "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/calendar.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n",
      "/kaggle/input/m5-target-encoding2/te_60.pkl\n",
      "/kaggle/input/m5-target-encoding2/__results__.html\n",
      "/kaggle/input/m5-target-encoding2/__notebook__.ipynb\n",
      "/kaggle/input/m5-target-encoding2/custom.css\n",
      "/kaggle/input/m5-target-encoding2/__output__.json\n",
      "/kaggle/input/m5-lags-features-1to35-ca/__results__.html\n",
      "/kaggle/input/m5-lags-features-1to35-ca/__notebook__.ipynb\n",
      "/kaggle/input/m5-lags-features-1to35-ca/custom.css\n",
      "/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_1.pkl\n",
      "/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_2.pkl\n",
      "/kaggle/input/m5-lags-features-1to35-ca/__output__.json\n",
      "/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_4.pkl\n",
      "/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_3.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_3.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_1.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/__results__.html\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/__notebook__.ipynb\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/custom.css\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/__output__.json\n",
      "/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_2.pkl\n",
      "/kaggle/input/m5-all-data/df_CA_1.pkl\n",
      "/kaggle/input/m5-all-data/df_TX_3.pkl\n",
      "/kaggle/input/m5-all-data/df_TX_2.pkl\n",
      "/kaggle/input/m5-all-data/df_CA_2.pkl\n",
      "/kaggle/input/m5-all-data/df_WI_3.pkl\n",
      "/kaggle/input/m5-all-data/df_CA_4.pkl\n",
      "/kaggle/input/m5-all-data/df_TX_1.pkl\n",
      "/kaggle/input/m5-all-data/df_WI_1.pkl\n",
      "/kaggle/input/m5-all-data/df_CA_3.pkl\n",
      "/kaggle/input/m5-all-data/df_WI_2.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_4.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/__results__.html\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_3.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/__notebook__.ipynb\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/custom.css\n",
      "/kaggle/input/binary-challenge-evaluation-ca-3-4/__output__.json\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/__results__.html\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_2.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_1.pkl\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/__notebook__.ipynb\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/custom.css\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/__output__.json\n",
      "/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_3.pkl\n",
      "/kaggle/input/kernel1f1484cf46/event_lag_df.pkl\n",
      "/kaggle/input/kernel1f1484cf46/__results__.html\n",
      "/kaggle/input/kernel1f1484cf46/__notebook__.ipynb\n",
      "/kaggle/input/kernel1f1484cf46/custom.css\n",
      "/kaggle/input/kernel1f1484cf46/__output__.json\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgb_optuna\n",
    "#import dask_xgboost as xgb\n",
    "#import dask.dataframe as dd6\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6369f9e7-0056-4cf6-9c38-45f8671a890c",
    "_uuid": "72a2bdc4-9107-4a5d-b195-1d900a75092a"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "82d615fb-4477-4d77-b437-d250775e7f00",
    "_uuid": "cf956280-e30a-4817-807d-a92b6d545304"
   },
   "source": [
    "## Use Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "8c8d5e45-3cad-4f46-ab57-af93bb7bc82f",
    "_uuid": "22b8ccdf-fd13-4ee2-a590-fdb956280bc1"
   },
   "outputs": [],
   "source": [
    "PATHS = {}\n",
    "for store_id in ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']:\n",
    "    PATHS[store_id] = '/kaggle/input/m5-all-data/df_' + store_id + '.pkl'\n",
    "    \n",
    "PATHS2 = {}\n",
    "PATHS2['CA_1'] = '/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_1.pkl'\n",
    "PATHS2['CA_2'] = '/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_2.pkl'\n",
    "PATHS2['CA_3'] = '/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_3.pkl'\n",
    "PATHS2['CA_4'] = '/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_4.pkl'\n",
    "PATHS2['TX_1'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_1.pkl'\n",
    "PATHS2['TX_2'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_2.pkl'\n",
    "PATHS2['TX_3'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_3.pkl'\n",
    "PATHS2['WI_1'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_1.pkl'\n",
    "PATHS2['WI_2'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_2.pkl'\n",
    "PATHS2['WI_3'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_3.pkl'\n",
    "\n",
    "PATHS3 = {}\n",
    "PATHS3['CA_1'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_1.pkl'\n",
    "PATHS3['CA_2'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_2.pkl'\n",
    "PATHS3['CA_3'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_3.pkl'\n",
    "PATHS3['CA_4'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_4.pkl'\n",
    "PATHS3['TX_1'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_1.pkl'\n",
    "PATHS3['TX_2'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_2.pkl'\n",
    "PATHS3['TX_3'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_3.pkl'\n",
    "PATHS3['WI_1'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_1.pkl'\n",
    "PATHS3['WI_2'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_2.pkl'\n",
    "PATHS3['WI_3'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_3.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "te2 = pd.read_pickle('../input/m5-target-encoding2/te_60.pkl')\n",
    "event_lag = pd.read_pickle('../input/kernel1f1484cf46/event_lag_df.pkl').drop(columns=['sales', 'event_name_1','event_lag_0'])\n",
    "\n",
    "def load_data(store_id):\n",
    "    df1 = pd.read_pickle(PATHS[store_id])\n",
    "    df2 = pd.read_pickle(PATHS2[store_id])\n",
    "    df3 = pd.read_pickle(PATHS3[store_id]).drop(columns=['store_id', 'sales']).iloc[:,:29]\n",
    "    df1 = df1.merge(df2,on=['id', 'd'],how='left')\n",
    "    df1 = df1.merge(df3,on=['id', 'd'],how='left')\n",
    "    df1 = df1.merge(te2,on=['id', 'd'],how='left')\n",
    "    df1 = df1.merge(event_lag,on=['id', 'd'],how='left')\n",
    "    del df2,df3\n",
    "    df1.id = df1.id.astype('category')\n",
    "    gc.collect()\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f039978-013c-4fd6-ae34-c2f9548912b5",
    "_uuid": "740adcda-49e9-43e7-bd60-5a6559016bd9"
   },
   "source": [
    "## Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "85b60c2c-f7bb-495e-8511-f7ae6830f79e",
    "_uuid": "3598c9f8-ba06-478f-be2c-30188ad84d6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = 'sales'\n",
    "\n",
    "basic_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
    "        'release', 'sell_price', 'price_max', 'price_min', 'price_std',\n",
    "       'price_mean', 'price_norm', 'price_nunique', 'item_nunique',\n",
    "       'price_momentum', 'price_momentum_m', 'price_momentum_y',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y',\n",
    "       'tm_wm', 'tm_dw', 'tm_w_end']\n",
    "\n",
    "encoding_features = ['te_id_28', 'te_item_id_28', 'te_dept_id_28', 'te_cat_id_28',\n",
    "       'te_store_id_28', 'te_state_id_28', 'te_id_tm_dw_28',\n",
    "       'te_item_id_tm_dw_28', 'te_dept_id_tm_dw_28', 'te_cat_id_tm_dw_28',\n",
    "       'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28'] + [\n",
    "       'te_id_60', 'te_item_id_60',\n",
    "       'te_dept_id_60', 'te_cat_id_60', 'te_store_id_60', 'te_state_id_60',\n",
    "       'te_id_tm_dw_60', 'te_item_id_tm_dw_60', 'te_dept_id_tm_dw_60',\n",
    "       'te_cat_id_tm_dw_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60']\n",
    "\n",
    "lag_features = [\n",
    "        'sales_lag_28','sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32',\n",
    "        'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36','sales_lag_37',\n",
    "        'sales_lag_38', 'sales_lag_39', 'sales_lag_40','sales_lag_41', 'sales_lag_42', \n",
    "        'rolling_mean_7', 'rolling_std_7','rolling_mean_14', 'rolling_std_14',\n",
    "        'rolling_mean_30','rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n",
    "        'rolling_mean_180', 'rolling_std_180',]\n",
    "\n",
    "day_by_day = ['sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4',\n",
    "       'sales_lag_5', 'sales_lag_6', 'sales_lag_7', 'sales_lag_8',\n",
    "       'sales_lag_9', 'sales_lag_10', 'sales_lag_11', 'sales_lag_12',\n",
    "       'sales_lag_13', 'sales_lag_14', 'sales_lag_15', 'sales_lag_16',\n",
    "       'sales_lag_17', 'sales_lag_18', 'sales_lag_19', 'sales_lag_20',\n",
    "       'sales_lag_21', 'sales_lag_22', 'sales_lag_23', 'sales_lag_24',\n",
    "       'sales_lag_25', 'sales_lag_26', 'sales_lag_27']\n",
    "\n",
    "recursive_features =['rolling_mean_tmp_1_7','rolling_mean_tmp_1_14',\n",
    "    'rolling_mean_tmp_1_30','rolling_mean_tmp_1_60',\n",
    "    'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14',\n",
    "    'rolling_mean_tmp_7_30','rolling_mean_tmp_7_60',\n",
    "    'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14',\n",
    "    'rolling_mean_tmp_14_30','rolling_mean_tmp_14_60']\n",
    "\n",
    "\t\n",
    "\n",
    "additional_features = ['binary_pred'] + ['event_lag_1','event_lag_2','event_lag_3','event_lag_4','event_lag_5','event_lag_6',\n",
    "     'event_lag_-7','event_lag_-6','event_lag_-5','event_lag_-4','event_lag_-3','event_lag_-2','event_lag_-1'] + day_by_day\n",
    "\n",
    "remove_features = ['store_id', 'state_id', \n",
    "                   'te_store_id_28', 'te_state_id_28', 'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28',\n",
    "                   'te_store_id_60', 'te_state_id_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60',\n",
    "                   'snap_CA', 'snap_TX', 'snap_WI']\n",
    "\n",
    "use_enc_feat = True\n",
    "use_lag_feat = True\n",
    "use_rec_feat = False\n",
    "use_add_feat = True\n",
    "\n",
    "feature = basic_features\n",
    "if use_enc_feat:\n",
    "    feature += encoding_features\n",
    "if use_lag_feat:\n",
    "    feature += lag_features\n",
    "if use_rec_feat:\n",
    "    feature += recursive_features\n",
    "if use_add_feat:\n",
    "    feature += additional_features\n",
    "    \n",
    "feature = [i for i in feature if i not in remove_features]\n",
    "    \n",
    "len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'item_id',\n",
       " 'dept_id',\n",
       " 'cat_id',\n",
       " 'release',\n",
       " 'sell_price',\n",
       " 'price_max',\n",
       " 'price_min',\n",
       " 'price_std',\n",
       " 'price_mean',\n",
       " 'price_norm',\n",
       " 'price_nunique',\n",
       " 'item_nunique',\n",
       " 'price_momentum',\n",
       " 'price_momentum_m',\n",
       " 'price_momentum_y',\n",
       " 'event_name_1',\n",
       " 'event_type_1',\n",
       " 'event_name_2',\n",
       " 'event_type_2',\n",
       " 'tm_d',\n",
       " 'tm_w',\n",
       " 'tm_m',\n",
       " 'tm_y',\n",
       " 'tm_wm',\n",
       " 'tm_dw',\n",
       " 'tm_w_end',\n",
       " 'te_id_28',\n",
       " 'te_item_id_28',\n",
       " 'te_dept_id_28',\n",
       " 'te_cat_id_28',\n",
       " 'te_id_tm_dw_28',\n",
       " 'te_item_id_tm_dw_28',\n",
       " 'te_dept_id_tm_dw_28',\n",
       " 'te_cat_id_tm_dw_28',\n",
       " 'te_id_60',\n",
       " 'te_item_id_60',\n",
       " 'te_dept_id_60',\n",
       " 'te_cat_id_60',\n",
       " 'te_id_tm_dw_60',\n",
       " 'te_item_id_tm_dw_60',\n",
       " 'te_dept_id_tm_dw_60',\n",
       " 'te_cat_id_tm_dw_60',\n",
       " 'sales_lag_28',\n",
       " 'sales_lag_29',\n",
       " 'sales_lag_30',\n",
       " 'sales_lag_31',\n",
       " 'sales_lag_32',\n",
       " 'sales_lag_33',\n",
       " 'sales_lag_34',\n",
       " 'sales_lag_35',\n",
       " 'sales_lag_36',\n",
       " 'sales_lag_37',\n",
       " 'sales_lag_38',\n",
       " 'sales_lag_39',\n",
       " 'sales_lag_40',\n",
       " 'sales_lag_41',\n",
       " 'sales_lag_42',\n",
       " 'rolling_mean_7',\n",
       " 'rolling_std_7',\n",
       " 'rolling_mean_14',\n",
       " 'rolling_std_14',\n",
       " 'rolling_mean_30',\n",
       " 'rolling_std_30',\n",
       " 'rolling_mean_60',\n",
       " 'rolling_std_60',\n",
       " 'rolling_mean_180',\n",
       " 'rolling_std_180',\n",
       " 'binary_pred',\n",
       " 'event_lag_1',\n",
       " 'event_lag_2',\n",
       " 'event_lag_3',\n",
       " 'event_lag_4',\n",
       " 'event_lag_5',\n",
       " 'event_lag_6',\n",
       " 'event_lag_-7',\n",
       " 'event_lag_-6',\n",
       " 'event_lag_-5',\n",
       " 'event_lag_-4',\n",
       " 'event_lag_-3',\n",
       " 'event_lag_-2',\n",
       " 'event_lag_-1',\n",
       " 'sales_lag_1',\n",
       " 'sales_lag_2',\n",
       " 'sales_lag_3',\n",
       " 'sales_lag_4',\n",
       " 'sales_lag_5',\n",
       " 'sales_lag_6',\n",
       " 'sales_lag_7',\n",
       " 'sales_lag_8',\n",
       " 'sales_lag_9',\n",
       " 'sales_lag_10',\n",
       " 'sales_lag_11',\n",
       " 'sales_lag_12',\n",
       " 'sales_lag_13',\n",
       " 'sales_lag_14',\n",
       " 'sales_lag_15',\n",
       " 'sales_lag_16',\n",
       " 'sales_lag_17',\n",
       " 'sales_lag_18',\n",
       " 'sales_lag_19',\n",
       " 'sales_lag_20',\n",
       " 'sales_lag_21',\n",
       " 'sales_lag_22',\n",
       " 'sales_lag_23',\n",
       " 'sales_lag_24',\n",
       " 'sales_lag_25',\n",
       " 'sales_lag_26',\n",
       " 'sales_lag_27']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9863ff87-ef18-45d2-b984-e8bb98126694",
    "_uuid": "6f5dda14-04ea-4801-95d2-766d112d23a2"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "5ed506ac-f878-4054-a37d-fa7266eaecf1",
    "_uuid": "1fb39bea-cc32-437d-bb64-656b5200d259"
   },
   "outputs": [],
   "source": [
    "# define lgbm simple model using custom loss and eval metric for early stopping\n",
    "def run_lgb(train, val,test, features, custom_loss, custom_eval, optuna_params={}, use_custom=True):\n",
    "\n",
    "    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n",
    "    del train\n",
    "    gc.collect()\n",
    "    \n",
    "    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n",
    "    del val\n",
    "    gc.collect()\n",
    "    \n",
    "    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    if use_custom:\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'first_metric_only': True,\n",
    "            'objective': 'custom',\n",
    "            'metric': 'custom',\n",
    "#             'n_jobs': -1,\n",
    "#             'seed': 42,\n",
    "#             'lambda_l1': 0.011947955379673579,\n",
    "#             'lambda_l2': 0.0002267728823544454,\n",
    "#             'num_leaves': 31,\n",
    "#             'feature_fraction': 0.45199999999999996,\n",
    "#              'bagging_fraction': 1.0,\n",
    "#              'bagging_freq': 0,\n",
    "#              'min_child_samples': 50,\n",
    "#             'learning_rate': 0.1,\n",
    "#             'n_estimators': 20,\n",
    "#             'bagging_fraction': 0.75,\n",
    "#             'bagging_freq': 10, \n",
    "#             'colsample_bytree': 0.75\n",
    "        }\n",
    "        params.update(optuna_params)\n",
    "        \n",
    "        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n",
    "                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n",
    "                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval)\n",
    "    else:\n",
    "        params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'first_metric_only': True,\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'custom',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 10, \n",
    "        'colsample_bytree': 0.75}\n",
    "        params.update(optuna_params)\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n",
    "                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n",
    "                          verbose_eval = 10,feval = custom_eval)\n",
    "    return model\n",
    "\n",
    "# define lgbm simple model using custom loss and eval metric for early stopping\n",
    "def run_lgb_no_early_stopping(train, features, custom_loss, custom_eval, optuna_params={}, use_custom=True, num_boost_round = 200):\n",
    "\n",
    "    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n",
    "    del train\n",
    "    gc.collect()\n",
    "\n",
    "    if use_custom:\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'first_metric_only': True,\n",
    "            'objective': 'custom',\n",
    "            'metric': 'rmse',\n",
    "#             'n_jobs': -1,\n",
    "#             'seed': 42,\n",
    "#             'learning_rate': 0.1,\n",
    "# #             'n_estimators': 20,\n",
    "#             'bagging_fraction': 0.75,\n",
    "#             'bagging_freq': 10, \n",
    "#             'colsample_bytree': 0.75\n",
    "        }\n",
    "        params.update(optuna_params)\n",
    "        \n",
    "        model = lgb.train(params, train_set, valid_sets = [train_set], num_boost_round = num_boost_round, verbose_eval = 10, fobj = custom_loss)\n",
    "    else:\n",
    "        params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'first_metric_only': True,\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'custom',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 10, \n",
    "        'colsample_bytree': 0.75}\n",
    "        params.update(optuna_params)\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = num_boost_round, early_stopping_rounds = 100, \n",
    "                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n",
    "                          verbose_eval = 10,feval = custom_eval)\n",
    "    return model\n",
    "\n",
    "# define lgbm simple model using custom loss and eval metric for early stopping\n",
    "def run_lgb_optuna(train, val,test, features, custom_loss, custom_eval, use_custom=True):\n",
    "\n",
    "    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n",
    "    del train\n",
    "    gc.collect()\n",
    "    \n",
    "    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n",
    "    del val\n",
    "    gc.collect()\n",
    "    \n",
    "    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "    best_params, history = {}, []\n",
    "\n",
    "    if use_custom:\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'first_metric_only': True,\n",
    "            'objective': 'custom',\n",
    "            'metric': 'wrmsse',\n",
    "            }\n",
    "        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n",
    "                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n",
    "                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval,best_params=best_params,tuning_history=history)\n",
    "    else:\n",
    "        params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'first_metric_only': True,\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'custom'\n",
    "        }\n",
    "        \n",
    "        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n",
    "                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n",
    "                          verbose_eval = 10,feval = custom_eval,best_params=best_params,tuning_history=history)\n",
    "    return model, best_params, history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1681ca21-9798-4e5e-9383-2b967d7260d7",
    "_uuid": "21935284-3bc8-46d5-a352-e539c5f8518b"
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "07f0a665-f4b8-4cb4-a8a3-11b210283fe6",
    "_uuid": "82982217-f835-4ff2-80dc-d6fdf4d55704"
   },
   "outputs": [],
   "source": [
    "# define cost and eval functions\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    a = 1.15\n",
    "    b = 1\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual * b, -2 * residual * a)\n",
    "    hess = np.where(residual < 0, 2 * b, 2 * a)\n",
    "    return grad, hess\n",
    "\n",
    "def tweedie(y_pred, y_true):\n",
    "    p = 1.1\n",
    "    y_true = y_true.get_label()\n",
    "    grad = -y_true*y_pred**(-p) + y_pred**(1-p)\n",
    "    hess = p*y_true*y_pred**(-p-1)-(1-p)*y_pred**(-p)\n",
    "    return grad, hess\n",
    "\n",
    "def tweedie2(y_pred, y_true):\n",
    "    p = 1.09\n",
    "    y_true = y_true.get_label()\n",
    "    grad = -y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p))\n",
    "    hess = -(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p))\n",
    "    return grad, hess\n",
    "\n",
    "def tweedie3(y_pred, y_true):\n",
    "    p = y_true.get_data()['p'].values\n",
    "    print(p)\n",
    "    y_true = y_true.get_label()\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    grad = np.where(p<1,-y_true + np.exp(y_pred),-y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p)))\n",
    "    grad = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),grad)\n",
    "    hess = np.where(p<1,np.exp(y_pred),-(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p)))\n",
    "    hess = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),hess)\n",
    "    print(grad,hess)\n",
    "    return grad, hess\n",
    "\n",
    "def tweedie_sum(y_pred, y_true):\n",
    "    p1 = 1.1\n",
    "    p2 = 1.5\n",
    "    y_true = y_true.get_label()\n",
    "    grad = -y_true*np.exp(y_pred*(1-p1))+np.exp(y_pred*(2-p1))\n",
    "    grad += -y_true*np.exp(y_pred*(1-p2))+np.exp(y_pred*(2-p2))\n",
    "    hess = -(1-p1)*y_true*np.exp(y_pred*(1-p1))+(2-p1)*np.exp(y_pred*(2-p1))\n",
    "    hess += -(1-p2)*y_true*np.exp(y_pred*(1-p2))+(2-p2)*np.exp(y_pred*(2-p2))\n",
    "#     print(grad,hess)\n",
    "    return grad, hess\n",
    "\n",
    "# define cost and eval functions\n",
    "def custom_asymmetric_train_2(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    grad = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), grad)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    hess = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), hess)\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "def zero_inflated_poisson_loss(y_pred, y_true):\n",
    "    p = 0.3\n",
    "    y_true = y_true.get_label()\n",
    "    y_pred = np.exp(y_pred)\n",
    "    grad = np.where(y_true == 0, (1-p)*y_pred,-y_true + y_pred)\n",
    "    hess = np.where(y_true == 0, (1-p)*y_pred, y_pred)\n",
    "    return grad, hess\n",
    "\n",
    "def poisson(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    y_pred = np.exp(y_pred)\n",
    "    grad = -y_true + y_pred\n",
    "    hess = y_pred\n",
    "    return grad, hess\n",
    "\n",
    "def custom_a(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    d = y_pred - y_true \n",
    "    grad = np.tanh(d)/y_true\n",
    "    hess = (1.0 - grad*grad)/y_true\n",
    "    return grad, hess\n",
    "\n",
    "def my_objective(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    d = y_pred - y_true \n",
    "    grad = 2*d\n",
    "    hess = 2*d/d\n",
    "    return grad, hess\n",
    "\n",
    "def loglikelood(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1. - preds)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def asymmetric_plus_tweedie(y_pred, y_true):\n",
    "    p = 1.06\n",
    "    a = 1.2\n",
    "    b = 1\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    y_pred = np.exp(y_pred)\n",
    "    grad = -y_true*y_pred**(1-p)+y_pred**(2-p)\n",
    "    grad += np.where(residual < 0, -2 * (y_true-y_pred)*y_pred * b, -2 * (y_true-y_pred)*y_pred * a)\n",
    "    hess = -(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p)\n",
    "    hess += np.where(residual < 0, -2 * (y_true-2*y_pred)*y_pred * b, -2 * (y_true-2*y_pred)*y_pred * a)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def asymmetric_tweedie(y_pred, y_true):\n",
    "    p = 1.0\n",
    "    a = 1.2\n",
    "    b = 1\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    y_pred = np.exp(y_pred)\n",
    "    grad = np.where(residual < 0,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * b,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * a)\n",
    "    hess = np.where(residual < 0,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*b,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*a)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3d4acf5d-af6e-43f9-9bb1-4178b819fccf",
    "_uuid": "6f923b89-c377-405f-858f-5ff40bd494b2"
   },
   "outputs": [],
   "source": [
    "def link_exp(pred):\n",
    "    return np.exp(pred)\n",
    "def link_normal(pred):\n",
    "    return pred\n",
    "link_func_dict = {'tweedie2':link_exp,'custom_asymmetric_train':link_normal,'asymmetric_plus_tweedie':link_exp,'asymmetric_tweedie':link_exp}\n",
    "loss_func_dict = {'tweedie2':tweedie2,'custom_asymmetric_train':custom_asymmetric_train,'asymmetric_plus_tweedie':asymmetric_plus_tweedie,'asymmetric_tweedie':asymmetric_tweedie}\n",
    "LOSS_NAME = 'tweedie2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a6aa270-c081-4643-b713-7c430db54f7a",
    "_uuid": "2cb971c9-6467-4df0-a9f9-ae6e3bfa97d6"
   },
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5b206bad-18d3-471d-a119-c5a85cfbb3cd",
    "_uuid": "4783e392-4619-4648-b22d-f8a028d9f22e"
   },
   "outputs": [],
   "source": [
    "def get_weight_mat(product):\n",
    "    NUM_ITEMS = len(product['id'].unique()) \n",
    "    weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n",
    "                       pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n",
    "                       ].T\n",
    "\n",
    "    weight_mat_csr = csr_matrix(weight_mat)\n",
    "    del weight_mat; gc.collect()\n",
    "    return weight_mat_csr\n",
    "\n",
    "def weight1_calc(product,weight_mat_csr):\n",
    "    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n",
    "    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "    sales_train_val = sales_train_val.set_index('id')\n",
    "    sales_train_val = sales_train_val.loc[product.id]\n",
    "    d_name = ['d_' + str(i+1) for i in range(1941)]\n",
    "    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n",
    "    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n",
    "    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n",
    "    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))\n",
    "    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n",
    "    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))<1\n",
    "    sales_train_val = np.where(flag,np.nan,sales_train_val)\n",
    "    # denominator of RMSSE / RMSSEの分母\n",
    "    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1941-start_no)\n",
    "\n",
    "    del sales_train_val\n",
    "    gc.collect()\n",
    "    \n",
    "    return weight1\n",
    "\n",
    "def weight2_calc(data_v,product,weight_mat_csr):\n",
    "    # calculate the sales amount for each item/level\n",
    "    df_tmp = data_v[['id','sales','sell_price']]\n",
    "    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n",
    "    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "    df_tmp = df_tmp[product.id].values\n",
    "    weight2 = weight_mat_csr * df_tmp \n",
    "    weight2 = weight2/np.sum(weight2)\n",
    "    \n",
    "    return  weight2\n",
    "\n",
    "def weight2_calc_for_train(data_v,product,weight_mat_csr):\n",
    "    # calculate the sales amount for each item/level\n",
    "    df_tmp = data_v[['id','sales','sell_price']]\n",
    "    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n",
    "    df_tmp.amount = df_tmp.amount.astype(int)\n",
    "    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "    df_tmp = df_tmp[product.id].values\n",
    "    weight2 = weight_mat_csr * df_tmp \n",
    "    weight2 = weight2/np.sum(weight2)\n",
    "    \n",
    "    return  weight2\n",
    "\n",
    "def wrmsse(preds, data):\n",
    "    \n",
    "    # this function is calculate for last 28 days to consider the non-zero demand period\n",
    "    \n",
    "    y_true = data.get_label()\n",
    "    if use_custom_loss:\n",
    "        preds = link_func_dict[LOSS_NAME](preds)\n",
    "    \n",
    "    if data.params['data_type']=='train':\n",
    "        return 'wrmsse', 1371, False\n",
    "        weight2 = weight2_tr\n",
    "        MASK = np.isin(np.arange(train_width_date*NUM_ITEMS),lost_days)\n",
    "\n",
    "        pred_tmp = np.arange(train_width_date*NUM_ITEMS)\n",
    "        pred_tmp[MASK] = 0\n",
    "        pred_tmp[~MASK] = preds\n",
    "        preds = pred_tmp\n",
    "        \n",
    "        y_true_tmp = np.arange(train_width_date*NUM_ITEMS)\n",
    "        y_true_tmp[MASK] = 0\n",
    "        y_true_tmp[~MASK] = y_true\n",
    "        y_true = y_true_tmp\n",
    "        \n",
    "    elif data.params['data_type']=='validation':\n",
    "        weight2 = weight2_val\n",
    "    elif data.params['data_type']=='test':\n",
    "        weight2 = weight2_te\n",
    "\n",
    "    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n",
    "    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n",
    "          \n",
    "    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(train)\n",
    "                        ,axis=1) / weight1) * weight2)\n",
    "#     print(np.mean(np.sqrt(np.mean(np.square(reshaped_preds-reshaped_true),axis=1))))\n",
    "    return 'wrmsse', score, False\n",
    "\n",
    "def wrmsse_sub(preds, y_true):\n",
    "              \n",
    "    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n",
    "    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n",
    "    if use_custom_loss:\n",
    "        preds = link_func_dict[LOSS_NAME](preds)\n",
    "        \n",
    "    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n",
    "\n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(train)\n",
    "                        ,axis=1) / weight1) * weight2)\n",
    "\n",
    "    return 'wrmsse', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "eec56f51-0c11-4e75-b055-9a6ec05f7c77",
    "_uuid": "c24c3fe9-90cd-473b-a84b-660c4d70b376"
   },
   "outputs": [],
   "source": [
    "def rmse(preds, data):\n",
    "    y_true = data.get_label()\n",
    "    if use_custom_loss:\n",
    "        preds = link_func_dict[LOSS_NAME](preds)\n",
    "                  \n",
    "    train = preds - y_true\n",
    "    \n",
    "    score = np.mean(np.sqrt(np.mean(np.square(train))))\n",
    "    \n",
    "    return 'rmse', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "8995a384-eb12-4bf3-baa3-9edb7176cfb9",
    "_uuid": "a1a0b005-cb55-4077-ade0-4a0987a19a6f"
   },
   "outputs": [],
   "source": [
    "def metrics(preds, data):\n",
    "    \"\"\"複数の評価指標を計算するための関数\"\"\"\n",
    "    return [\n",
    "        wrmsse(preds, data),\n",
    "        rmse(preds, data)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a40be0b-8ade-43f4-8baa-0aed705ab0f7",
    "_uuid": "3f5316ba-2ea7-45f3-b57e-26e9d95d1997"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8de59f05-d823-4f0b-88dc-158fedf9c008",
    "_uuid": "42117479-9ac8-4775-878c-2e51a52821ff"
   },
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "f9764bf9-8aee-45de-8ed3-836405a6ca00",
    "_uuid": "9a8eed6f-55bc-4535-b801-ecdb30cbd328"
   },
   "outputs": [],
   "source": [
    "use_custom_loss = True\n",
    "\n",
    "test_start_date = 1941 - 28\n",
    "test_end_date = 1941\n",
    "\n",
    "train_width_date = 365 * 5\n",
    "val_width_date = 28\n",
    "shift_width_date = 28\n",
    "min_train_date = 0\n",
    "\n",
    "slide_list = []\n",
    "for i in range(test_start_date-1,1,-shift_width_date):\n",
    "    end_date = i\n",
    "    split_date = end_date - val_width_date\n",
    "    start_date = split_date - train_width_date\n",
    "    if start_date < min_train_date:\n",
    "        break\n",
    "    slide_list.append([start_date,split_date,end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "dab738c3-af32-4572-9944-18b611858a5d",
    "_uuid": "fcd356a7-80af-48e4-afc0-35f4fd47f97c"
   },
   "outputs": [],
   "source": [
    "# 5 times validation for fast notebook\n",
    "slide_list = slide_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "749e2334-347b-4b62-8b82-36862c8e6109",
    "_uuid": "265df82c-c9b7-45e3-b1f6-1d13ceff3ab1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[59, 1884, 1912], [31, 1856, 1884], [3, 1828, 1856]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "0e66cd79-6e69-470e-9673-5a27ee482c5d",
    "_uuid": "b2a82350-aefb-4869-acaf-c31b87ae12ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 s, sys: 2.77 s, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "product = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "product = product[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b46f5a1-a754-4e40-aa7f-c7dad297e9e1",
    "_uuid": "8f9d9f31-9393-4096-bcf2-aae435003546"
   },
   "source": [
    "## Make Parameter Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0d3b20d-b87d-4e55-8214-d0d60ecf07c9",
    "_uuid": "df96d529-ea51-462f-9832-c5e740221bd1"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "331d439b-9121-4830-abc2-105ff30bde54",
    "_uuid": "9b03d25f-6341-42ee-b59d-7bc3f51752f0"
   },
   "outputs": [],
   "source": [
    "STORE_IDS = list(product.store_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "51c948b4-42a4-4811-a9fa-ff7fbf2f5ac7",
    "_uuid": "9693b9fd-2eaf-4ffd-b16e-dbdd6cd7fc13"
   },
   "outputs": [],
   "source": [
    "# store_id = 'CA_1'\n",
    "# start_date, split_date, end_date = slide_list[0] \n",
    "# print('start_date, split_date, end_date:',start_date, split_date, end_date)\n",
    "# print('store_id:',store_id)\n",
    "# print('load dataset')\n",
    "# df = load_data(store_id)\n",
    "# day_mask = (df.d>=start_date)&(df.d<split_date)\n",
    "# train = df[day_mask]\n",
    "# train_ids = train.id.unique()\n",
    "# NUM_ITEMS = len(train_ids)\n",
    "\n",
    "# day_mask = (df.d>=split_date)&(df.d<=end_date)\n",
    "# val = df[day_mask]\n",
    "# val = val[val.id.isin(train_ids)]\n",
    "# day_mask = (df.d>=test_start_date)&(df.d<=test_end_date)\n",
    "# test = df[day_mask]\n",
    "# del df\n",
    "# gc.collect()\n",
    "\n",
    "# test_tmp = test[test.id.isin(train_ids)]\n",
    "# product_tmp = product[product.id.isin(train_ids)]\n",
    "\n",
    "# print('calc weight')\n",
    "# weight_mat_csr = get_weight_mat(product_tmp)\n",
    "# weight1 = weight1_calc(product_tmp,weight_mat_csr)\n",
    "# weight2_te = weight2_calc(test_tmp,product_tmp,weight_mat_csr)\n",
    "# weight2_val = weight2_calc(val,product_tmp,weight_mat_csr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"lambda_l1\":0.0000013771438547444856,\n",
    "\"lambda_l2\":1.8704380943506742,\n",
    "\"num_leaves\":27,\n",
    "\"feature_fraction\":0.8,\n",
    "\"bagging_fraction\":0.8154703815794264,\n",
    "\"bagging_freq\":1,\n",
    "\"min_child_samples\":20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PREDICT_DAY in range(1,29): \n",
    "# #        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n",
    "#     remove_lag = []\n",
    "#     for i in range(1, PREDICT_DAY):\n",
    "#         remove_lag.append('sales_lag_{}'.format(i))\n",
    "#     new_features = list(set(feature) - set(remove_lag))\n",
    "#     print('train model; day', PREDICT_DAY)\n",
    "#     print(remove_lag)\n",
    "#     model = run_lgb(train,val,test_tmp, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params)\n",
    "#    # model = run_lgb_no_early_stopping(train, new_features, loss_func_dict[LOSS_NAME], wrmsse, num_boost_round = num_boost_rounds[store_id])\n",
    "#     model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n",
    "#     pickle.dump(model, open(model_name, 'wb'))\n",
    "#     del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "ef328e02-ffab-4a5a-850e-ab840fa8154a",
    "_uuid": "82f3ecd0-2777-4275-ba82-a4ca486fecd3"
   },
   "outputs": [],
   "source": [
    "# num_boost_rounds = {\n",
    "#     'CA_1': 250,\n",
    "#     'CA_2': 50,\n",
    "#     'CA_3': 50,\n",
    "#     'CA_4': 100,\n",
    "#     'TX_1': 250,\n",
    "#     'TX_2': 50,    \n",
    "#     'TX_3': 50,\n",
    "#     'WI_1': 75,\n",
    "#     'WI_2': 75,\n",
    "#     'WI_3': 100,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "74f33800-7fe1-4a03-9183-1a63cedb6ccc",
    "_uuid": "e3848954-ef7f-4953-bd88-0fac6f81f220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id: CA_1\n",
      "load dataset\n",
      "train model; day 1\n",
      "[10]\ttraining's rmse: 4.38152\n",
      "[20]\ttraining's rmse: 4.37419\n",
      "[30]\ttraining's rmse: 4.40232\n",
      "[40]\ttraining's rmse: 4.43025\n",
      "[50]\ttraining's rmse: 4.44372\n",
      "[60]\ttraining's rmse: 4.45422\n",
      "[70]\ttraining's rmse: 4.4606\n",
      "[80]\ttraining's rmse: 4.46374\n",
      "[90]\ttraining's rmse: 4.46637\n",
      "[100]\ttraining's rmse: 4.47036\n",
      "[110]\ttraining's rmse: 4.472\n",
      "[120]\ttraining's rmse: 4.47446\n",
      "[130]\ttraining's rmse: 4.47526\n",
      "[140]\ttraining's rmse: 4.47634\n",
      "[150]\ttraining's rmse: 4.4781\n",
      "[160]\ttraining's rmse: 4.47917\n",
      "[170]\ttraining's rmse: 4.47905\n",
      "[180]\ttraining's rmse: 4.48106\n",
      "[190]\ttraining's rmse: 4.4823\n",
      "[200]\ttraining's rmse: 4.48357\n",
      "[210]\ttraining's rmse: 4.4852\n",
      "[220]\ttraining's rmse: 4.48581\n",
      "[230]\ttraining's rmse: 4.48676\n",
      "[240]\ttraining's rmse: 4.48786\n",
      "[250]\ttraining's rmse: 4.48844\n",
      "[260]\ttraining's rmse: 4.48996\n",
      "[270]\ttraining's rmse: 4.49072\n",
      "[280]\ttraining's rmse: 4.492\n",
      "[290]\ttraining's rmse: 4.49177\n",
      "[300]\ttraining's rmse: 4.49274\n",
      "train model; day 2\n",
      "[10]\ttraining's rmse: 4.38646\n",
      "[20]\ttraining's rmse: 4.37468\n",
      "[30]\ttraining's rmse: 4.39904\n",
      "[40]\ttraining's rmse: 4.42376\n",
      "[50]\ttraining's rmse: 4.43744\n",
      "[60]\ttraining's rmse: 4.44723\n",
      "[70]\ttraining's rmse: 4.45314\n",
      "[80]\ttraining's rmse: 4.45832\n",
      "[90]\ttraining's rmse: 4.46109\n",
      "[100]\ttraining's rmse: 4.46267\n",
      "[110]\ttraining's rmse: 4.46628\n",
      "[120]\ttraining's rmse: 4.46903\n",
      "[130]\ttraining's rmse: 4.47007\n",
      "[140]\ttraining's rmse: 4.47163\n",
      "[150]\ttraining's rmse: 4.47244\n",
      "[160]\ttraining's rmse: 4.47409\n",
      "[170]\ttraining's rmse: 4.47648\n",
      "[180]\ttraining's rmse: 4.47775\n",
      "[190]\ttraining's rmse: 4.47826\n",
      "[200]\ttraining's rmse: 4.47934\n",
      "[210]\ttraining's rmse: 4.48005\n",
      "[220]\ttraining's rmse: 4.48053\n",
      "[230]\ttraining's rmse: 4.48088\n",
      "[240]\ttraining's rmse: 4.48183\n",
      "[250]\ttraining's rmse: 4.48243\n",
      "[260]\ttraining's rmse: 4.48339\n",
      "[270]\ttraining's rmse: 4.48391\n",
      "[280]\ttraining's rmse: 4.48423\n",
      "[290]\ttraining's rmse: 4.48532\n",
      "[300]\ttraining's rmse: 4.48593\n",
      "train model; day 3\n",
      "[10]\ttraining's rmse: 4.38859\n",
      "[20]\ttraining's rmse: 4.37614\n",
      "[30]\ttraining's rmse: 4.3979\n",
      "[40]\ttraining's rmse: 4.4189\n",
      "[50]\ttraining's rmse: 4.43096\n",
      "[60]\ttraining's rmse: 4.44035\n",
      "[70]\ttraining's rmse: 4.44713\n",
      "[80]\ttraining's rmse: 4.45157\n",
      "[90]\ttraining's rmse: 4.45472\n",
      "[100]\ttraining's rmse: 4.45608\n",
      "[110]\ttraining's rmse: 4.45813\n",
      "[120]\ttraining's rmse: 4.45932\n",
      "[130]\ttraining's rmse: 4.46084\n",
      "[140]\ttraining's rmse: 4.46229\n",
      "[150]\ttraining's rmse: 4.46333\n",
      "[160]\ttraining's rmse: 4.46406\n",
      "[170]\ttraining's rmse: 4.46663\n",
      "[180]\ttraining's rmse: 4.4675\n",
      "[190]\ttraining's rmse: 4.46823\n",
      "[200]\ttraining's rmse: 4.47\n",
      "[210]\ttraining's rmse: 4.47078\n",
      "[220]\ttraining's rmse: 4.47201\n",
      "[230]\ttraining's rmse: 4.47266\n",
      "[240]\ttraining's rmse: 4.47321\n",
      "[250]\ttraining's rmse: 4.47386\n",
      "[260]\ttraining's rmse: 4.47529\n",
      "[270]\ttraining's rmse: 4.47676\n",
      "[280]\ttraining's rmse: 4.47731\n",
      "[290]\ttraining's rmse: 4.47776\n",
      "[300]\ttraining's rmse: 4.479\n",
      "train model; day 4\n",
      "[10]\ttraining's rmse: 4.38965\n",
      "[20]\ttraining's rmse: 4.37672\n",
      "[30]\ttraining's rmse: 4.3967\n",
      "[40]\ttraining's rmse: 4.41794\n",
      "[50]\ttraining's rmse: 4.42861\n",
      "[60]\ttraining's rmse: 4.43756\n",
      "[70]\ttraining's rmse: 4.44197\n",
      "[80]\ttraining's rmse: 4.44617\n",
      "[90]\ttraining's rmse: 4.44908\n",
      "[100]\ttraining's rmse: 4.4512\n",
      "[110]\ttraining's rmse: 4.4536\n",
      "[120]\ttraining's rmse: 4.45589\n",
      "[130]\ttraining's rmse: 4.45814\n",
      "[140]\ttraining's rmse: 4.4598\n",
      "[150]\ttraining's rmse: 4.46109\n",
      "[160]\ttraining's rmse: 4.46318\n",
      "[170]\ttraining's rmse: 4.46398\n",
      "[180]\ttraining's rmse: 4.46433\n",
      "[190]\ttraining's rmse: 4.46527\n",
      "[200]\ttraining's rmse: 4.46591\n",
      "[210]\ttraining's rmse: 4.46751\n",
      "[220]\ttraining's rmse: 4.46845\n",
      "[230]\ttraining's rmse: 4.46916\n",
      "[240]\ttraining's rmse: 4.47004\n",
      "[250]\ttraining's rmse: 4.47159\n",
      "[260]\ttraining's rmse: 4.47229\n",
      "[270]\ttraining's rmse: 4.47234\n",
      "[280]\ttraining's rmse: 4.4735\n",
      "[290]\ttraining's rmse: 4.47419\n",
      "[300]\ttraining's rmse: 4.47456\n",
      "train model; day 5\n",
      "[10]\ttraining's rmse: 4.39083\n",
      "[20]\ttraining's rmse: 4.37727\n",
      "[30]\ttraining's rmse: 4.39596\n",
      "[40]\ttraining's rmse: 4.41433\n",
      "[50]\ttraining's rmse: 4.42494\n",
      "[60]\ttraining's rmse: 4.43444\n",
      "[70]\ttraining's rmse: 4.44009\n",
      "[80]\ttraining's rmse: 4.44315\n",
      "[90]\ttraining's rmse: 4.447\n",
      "[100]\ttraining's rmse: 4.45093\n",
      "[110]\ttraining's rmse: 4.45208\n",
      "[120]\ttraining's rmse: 4.45432\n",
      "[130]\ttraining's rmse: 4.45698\n",
      "[140]\ttraining's rmse: 4.45809\n",
      "[150]\ttraining's rmse: 4.45908\n",
      "[160]\ttraining's rmse: 4.45978\n",
      "[170]\ttraining's rmse: 4.46181\n",
      "[180]\ttraining's rmse: 4.46208\n",
      "[190]\ttraining's rmse: 4.46438\n",
      "[200]\ttraining's rmse: 4.4652\n",
      "[210]\ttraining's rmse: 4.46557\n",
      "[220]\ttraining's rmse: 4.46736\n",
      "[230]\ttraining's rmse: 4.46807\n",
      "[240]\ttraining's rmse: 4.46896\n",
      "[250]\ttraining's rmse: 4.46944\n",
      "[260]\ttraining's rmse: 4.4702\n",
      "[270]\ttraining's rmse: 4.47102\n",
      "[280]\ttraining's rmse: 4.47186\n",
      "[290]\ttraining's rmse: 4.47228\n",
      "[300]\ttraining's rmse: 4.47289\n",
      "train model; day 6\n",
      "[10]\ttraining's rmse: 4.3912\n",
      "[20]\ttraining's rmse: 4.37705\n",
      "[30]\ttraining's rmse: 4.39492\n",
      "[40]\ttraining's rmse: 4.41195\n",
      "[50]\ttraining's rmse: 4.42419\n",
      "[60]\ttraining's rmse: 4.4307\n",
      "[70]\ttraining's rmse: 4.43747\n",
      "[80]\ttraining's rmse: 4.4407\n",
      "[90]\ttraining's rmse: 4.44467\n",
      "[100]\ttraining's rmse: 4.44727\n",
      "[110]\ttraining's rmse: 4.44977\n",
      "[120]\ttraining's rmse: 4.45214\n",
      "[130]\ttraining's rmse: 4.45455\n",
      "[140]\ttraining's rmse: 4.45588\n",
      "[150]\ttraining's rmse: 4.45743\n",
      "[160]\ttraining's rmse: 4.45788\n",
      "[170]\ttraining's rmse: 4.45933\n",
      "[180]\ttraining's rmse: 4.45989\n",
      "[190]\ttraining's rmse: 4.46055\n",
      "[200]\ttraining's rmse: 4.46117\n",
      "[210]\ttraining's rmse: 4.46239\n",
      "[220]\ttraining's rmse: 4.4637\n",
      "[230]\ttraining's rmse: 4.46459\n",
      "[240]\ttraining's rmse: 4.46542\n",
      "[250]\ttraining's rmse: 4.46782\n",
      "[260]\ttraining's rmse: 4.46912\n",
      "[270]\ttraining's rmse: 4.46937\n",
      "[280]\ttraining's rmse: 4.47003\n",
      "[290]\ttraining's rmse: 4.47033\n",
      "[300]\ttraining's rmse: 4.47109\n",
      "train model; day 7\n",
      "[10]\ttraining's rmse: 4.39276\n",
      "[20]\ttraining's rmse: 4.37803\n",
      "[30]\ttraining's rmse: 4.39466\n",
      "[40]\ttraining's rmse: 4.41054\n",
      "[50]\ttraining's rmse: 4.4215\n",
      "[60]\ttraining's rmse: 4.42936\n",
      "[70]\ttraining's rmse: 4.43543\n",
      "[80]\ttraining's rmse: 4.44049\n",
      "[90]\ttraining's rmse: 4.44337\n",
      "[100]\ttraining's rmse: 4.44574\n",
      "[110]\ttraining's rmse: 4.44837\n",
      "[120]\ttraining's rmse: 4.45027\n",
      "[130]\ttraining's rmse: 4.45218\n",
      "[140]\ttraining's rmse: 4.45312\n",
      "[150]\ttraining's rmse: 4.4556\n",
      "[160]\ttraining's rmse: 4.45607\n",
      "[170]\ttraining's rmse: 4.4569\n",
      "[180]\ttraining's rmse: 4.45783\n",
      "[190]\ttraining's rmse: 4.45882\n",
      "[200]\ttraining's rmse: 4.45951\n",
      "[210]\ttraining's rmse: 4.46001\n",
      "[220]\ttraining's rmse: 4.46225\n",
      "[230]\ttraining's rmse: 4.4631\n",
      "[240]\ttraining's rmse: 4.46379\n",
      "[250]\ttraining's rmse: 4.46476\n",
      "[260]\ttraining's rmse: 4.46496\n",
      "[270]\ttraining's rmse: 4.46567\n",
      "[280]\ttraining's rmse: 4.46597\n",
      "[290]\ttraining's rmse: 4.46739\n",
      "[300]\ttraining's rmse: 4.4683\n",
      "train model; day 8\n",
      "[10]\ttraining's rmse: 4.39364\n",
      "[20]\ttraining's rmse: 4.37795\n",
      "[30]\ttraining's rmse: 4.3936\n",
      "[40]\ttraining's rmse: 4.40934\n",
      "[50]\ttraining's rmse: 4.42064\n",
      "[60]\ttraining's rmse: 4.42753\n",
      "[70]\ttraining's rmse: 4.43333\n",
      "[80]\ttraining's rmse: 4.43795\n",
      "[90]\ttraining's rmse: 4.44211\n",
      "[100]\ttraining's rmse: 4.44559\n",
      "[110]\ttraining's rmse: 4.44766\n",
      "[120]\ttraining's rmse: 4.44968\n",
      "[130]\ttraining's rmse: 4.45102\n",
      "[140]\ttraining's rmse: 4.45205\n",
      "[150]\ttraining's rmse: 4.45263\n",
      "[160]\ttraining's rmse: 4.45523\n",
      "[170]\ttraining's rmse: 4.4556\n",
      "[180]\ttraining's rmse: 4.45753\n",
      "[190]\ttraining's rmse: 4.45885\n",
      "[200]\ttraining's rmse: 4.46003\n",
      "[210]\ttraining's rmse: 4.46015\n",
      "[220]\ttraining's rmse: 4.46098\n",
      "[230]\ttraining's rmse: 4.46161\n",
      "[240]\ttraining's rmse: 4.46261\n",
      "[250]\ttraining's rmse: 4.46366\n",
      "[260]\ttraining's rmse: 4.4642\n",
      "[270]\ttraining's rmse: 4.46508\n",
      "[280]\ttraining's rmse: 4.46542\n",
      "[290]\ttraining's rmse: 4.46577\n",
      "[300]\ttraining's rmse: 4.46738\n",
      "train model; day 9\n",
      "[10]\ttraining's rmse: 4.39502\n",
      "[20]\ttraining's rmse: 4.37834\n",
      "[30]\ttraining's rmse: 4.39362\n",
      "[40]\ttraining's rmse: 4.4088\n",
      "[50]\ttraining's rmse: 4.41764\n",
      "[60]\ttraining's rmse: 4.42543\n",
      "[70]\ttraining's rmse: 4.43069\n",
      "[80]\ttraining's rmse: 4.43497\n",
      "[90]\ttraining's rmse: 4.43817\n",
      "[100]\ttraining's rmse: 4.44145\n",
      "[110]\ttraining's rmse: 4.444\n",
      "[120]\ttraining's rmse: 4.44553\n",
      "[130]\ttraining's rmse: 4.44801\n",
      "[140]\ttraining's rmse: 4.44916\n",
      "[150]\ttraining's rmse: 4.451\n",
      "[160]\ttraining's rmse: 4.45235\n",
      "[170]\ttraining's rmse: 4.45318\n",
      "[180]\ttraining's rmse: 4.45409\n",
      "[190]\ttraining's rmse: 4.45551\n",
      "[200]\ttraining's rmse: 4.45672\n",
      "[210]\ttraining's rmse: 4.45802\n",
      "[220]\ttraining's rmse: 4.4587\n",
      "[230]\ttraining's rmse: 4.45878\n",
      "[240]\ttraining's rmse: 4.45981\n",
      "[250]\ttraining's rmse: 4.46112\n",
      "[260]\ttraining's rmse: 4.46249\n",
      "[270]\ttraining's rmse: 4.46234\n",
      "[280]\ttraining's rmse: 4.46278\n",
      "[290]\ttraining's rmse: 4.46399\n",
      "[300]\ttraining's rmse: 4.46433\n",
      "train model; day 10\n",
      "[10]\ttraining's rmse: 4.3957\n",
      "[20]\ttraining's rmse: 4.37936\n",
      "[30]\ttraining's rmse: 4.39395\n",
      "[40]\ttraining's rmse: 4.40898\n",
      "[50]\ttraining's rmse: 4.41838\n",
      "[60]\ttraining's rmse: 4.42562\n",
      "[70]\ttraining's rmse: 4.4314\n",
      "[80]\ttraining's rmse: 4.43581\n",
      "[90]\ttraining's rmse: 4.44004\n",
      "[100]\ttraining's rmse: 4.44198\n",
      "[110]\ttraining's rmse: 4.4443\n",
      "[120]\ttraining's rmse: 4.44685\n",
      "[130]\ttraining's rmse: 4.44844\n",
      "[140]\ttraining's rmse: 4.45076\n",
      "[150]\ttraining's rmse: 4.45157\n",
      "[160]\ttraining's rmse: 4.45386\n",
      "[170]\ttraining's rmse: 4.45367\n",
      "[180]\ttraining's rmse: 4.45544\n",
      "[190]\ttraining's rmse: 4.45582\n",
      "[200]\ttraining's rmse: 4.45629\n",
      "[210]\ttraining's rmse: 4.45737\n",
      "[220]\ttraining's rmse: 4.45839\n",
      "[230]\ttraining's rmse: 4.45902\n",
      "[240]\ttraining's rmse: 4.45988\n",
      "[250]\ttraining's rmse: 4.46157\n",
      "[260]\ttraining's rmse: 4.46221\n",
      "[270]\ttraining's rmse: 4.46299\n",
      "[280]\ttraining's rmse: 4.46432\n",
      "[290]\ttraining's rmse: 4.46578\n",
      "[300]\ttraining's rmse: 4.46634\n",
      "train model; day 11\n",
      "[10]\ttraining's rmse: 4.39556\n",
      "[20]\ttraining's rmse: 4.37934\n",
      "[30]\ttraining's rmse: 4.39388\n",
      "[40]\ttraining's rmse: 4.40832\n",
      "[50]\ttraining's rmse: 4.41704\n",
      "[60]\ttraining's rmse: 4.4248\n",
      "[70]\ttraining's rmse: 4.43087\n",
      "[80]\ttraining's rmse: 4.43446\n",
      "[90]\ttraining's rmse: 4.43799\n",
      "[100]\ttraining's rmse: 4.44089\n",
      "[110]\ttraining's rmse: 4.44328\n",
      "[120]\ttraining's rmse: 4.44519\n",
      "[130]\ttraining's rmse: 4.44719\n",
      "[140]\ttraining's rmse: 4.44907\n",
      "[150]\ttraining's rmse: 4.45042\n",
      "[160]\ttraining's rmse: 4.45215\n",
      "[170]\ttraining's rmse: 4.4539\n",
      "[180]\ttraining's rmse: 4.45413\n",
      "[190]\ttraining's rmse: 4.45556\n",
      "[200]\ttraining's rmse: 4.45672\n",
      "[210]\ttraining's rmse: 4.45693\n",
      "[220]\ttraining's rmse: 4.45801\n",
      "[230]\ttraining's rmse: 4.45861\n",
      "[240]\ttraining's rmse: 4.45953\n",
      "[250]\ttraining's rmse: 4.46059\n",
      "[260]\ttraining's rmse: 4.46176\n",
      "[270]\ttraining's rmse: 4.46241\n",
      "[280]\ttraining's rmse: 4.4627\n",
      "[290]\ttraining's rmse: 4.4633\n",
      "[300]\ttraining's rmse: 4.46465\n",
      "train model; day 12\n",
      "[10]\ttraining's rmse: 4.39575\n",
      "[20]\ttraining's rmse: 4.37977\n",
      "[30]\ttraining's rmse: 4.39346\n",
      "[40]\ttraining's rmse: 4.40816\n",
      "[50]\ttraining's rmse: 4.41828\n",
      "[60]\ttraining's rmse: 4.42562\n",
      "[70]\ttraining's rmse: 4.43056\n",
      "[80]\ttraining's rmse: 4.43461\n",
      "[90]\ttraining's rmse: 4.43759\n",
      "[100]\ttraining's rmse: 4.44139\n",
      "[110]\ttraining's rmse: 4.44321\n",
      "[120]\ttraining's rmse: 4.44491\n",
      "[130]\ttraining's rmse: 4.44786\n",
      "[140]\ttraining's rmse: 4.45053\n",
      "[150]\ttraining's rmse: 4.45173\n",
      "[160]\ttraining's rmse: 4.45272\n",
      "[170]\ttraining's rmse: 4.45445\n",
      "[180]\ttraining's rmse: 4.45598\n",
      "[190]\ttraining's rmse: 4.45702\n",
      "[200]\ttraining's rmse: 4.4591\n",
      "[210]\ttraining's rmse: 4.45975\n",
      "[220]\ttraining's rmse: 4.45993\n",
      "[230]\ttraining's rmse: 4.46024\n",
      "[240]\ttraining's rmse: 4.4612\n",
      "[250]\ttraining's rmse: 4.4626\n",
      "[260]\ttraining's rmse: 4.4632\n",
      "[270]\ttraining's rmse: 4.46448\n",
      "[280]\ttraining's rmse: 4.4651\n",
      "[290]\ttraining's rmse: 4.46584\n",
      "[300]\ttraining's rmse: 4.46688\n",
      "train model; day 13\n",
      "[10]\ttraining's rmse: 4.39595\n",
      "[20]\ttraining's rmse: 4.38001\n",
      "[30]\ttraining's rmse: 4.39382\n",
      "[40]\ttraining's rmse: 4.40845\n",
      "[50]\ttraining's rmse: 4.41759\n",
      "[60]\ttraining's rmse: 4.42433\n",
      "[70]\ttraining's rmse: 4.42994\n",
      "[80]\ttraining's rmse: 4.43495\n",
      "[90]\ttraining's rmse: 4.43752\n",
      "[100]\ttraining's rmse: 4.43971\n",
      "[110]\ttraining's rmse: 4.4421\n",
      "[120]\ttraining's rmse: 4.44482\n",
      "[130]\ttraining's rmse: 4.44644\n",
      "[140]\ttraining's rmse: 4.44843\n",
      "[150]\ttraining's rmse: 4.4498\n",
      "[160]\ttraining's rmse: 4.4517\n",
      "[170]\ttraining's rmse: 4.45307\n",
      "[180]\ttraining's rmse: 4.45322\n",
      "[190]\ttraining's rmse: 4.45508\n",
      "[200]\ttraining's rmse: 4.4557\n",
      "[210]\ttraining's rmse: 4.45696\n",
      "[220]\ttraining's rmse: 4.45804\n",
      "[230]\ttraining's rmse: 4.45905\n",
      "[240]\ttraining's rmse: 4.46028\n",
      "[250]\ttraining's rmse: 4.46133\n",
      "[260]\ttraining's rmse: 4.46184\n",
      "[270]\ttraining's rmse: 4.46231\n",
      "[280]\ttraining's rmse: 4.46242\n",
      "[290]\ttraining's rmse: 4.46284\n",
      "[300]\ttraining's rmse: 4.46392\n",
      "train model; day 14\n",
      "[10]\ttraining's rmse: 4.39634\n",
      "[20]\ttraining's rmse: 4.38041\n",
      "[30]\ttraining's rmse: 4.3945\n",
      "[40]\ttraining's rmse: 4.40827\n",
      "[50]\ttraining's rmse: 4.41798\n",
      "[60]\ttraining's rmse: 4.42433\n",
      "[70]\ttraining's rmse: 4.42965\n",
      "[80]\ttraining's rmse: 4.43343\n",
      "[90]\ttraining's rmse: 4.43693\n",
      "[100]\ttraining's rmse: 4.4392\n",
      "[110]\ttraining's rmse: 4.44074\n",
      "[120]\ttraining's rmse: 4.44232\n",
      "[130]\ttraining's rmse: 4.44475\n",
      "[140]\ttraining's rmse: 4.44643\n",
      "[150]\ttraining's rmse: 4.44796\n",
      "[160]\ttraining's rmse: 4.44997\n",
      "[170]\ttraining's rmse: 4.4517\n",
      "[180]\ttraining's rmse: 4.45218\n",
      "[190]\ttraining's rmse: 4.45361\n",
      "[200]\ttraining's rmse: 4.4556\n",
      "[210]\ttraining's rmse: 4.45614\n",
      "[220]\ttraining's rmse: 4.45691\n",
      "[230]\ttraining's rmse: 4.45731\n",
      "[240]\ttraining's rmse: 4.45865\n",
      "[250]\ttraining's rmse: 4.46002\n",
      "[260]\ttraining's rmse: 4.46077\n",
      "[270]\ttraining's rmse: 4.46162\n",
      "[280]\ttraining's rmse: 4.46185\n",
      "[290]\ttraining's rmse: 4.46255\n",
      "[300]\ttraining's rmse: 4.4643\n",
      "train model; day 15\n",
      "[10]\ttraining's rmse: 4.39679\n",
      "[20]\ttraining's rmse: 4.38052\n",
      "[30]\ttraining's rmse: 4.39393\n",
      "[40]\ttraining's rmse: 4.40755\n",
      "[50]\ttraining's rmse: 4.41788\n",
      "[60]\ttraining's rmse: 4.42507\n",
      "[70]\ttraining's rmse: 4.42992\n",
      "[80]\ttraining's rmse: 4.43323\n",
      "[90]\ttraining's rmse: 4.43699\n",
      "[100]\ttraining's rmse: 4.43938\n",
      "[110]\ttraining's rmse: 4.4413\n",
      "[120]\ttraining's rmse: 4.44355\n",
      "[130]\ttraining's rmse: 4.44572\n",
      "[140]\ttraining's rmse: 4.44676\n",
      "[150]\ttraining's rmse: 4.44871\n",
      "[160]\ttraining's rmse: 4.44949\n",
      "[170]\ttraining's rmse: 4.45128\n",
      "[180]\ttraining's rmse: 4.45199\n",
      "[190]\ttraining's rmse: 4.4531\n",
      "[200]\ttraining's rmse: 4.45497\n",
      "[210]\ttraining's rmse: 4.45605\n",
      "[220]\ttraining's rmse: 4.45665\n",
      "[230]\ttraining's rmse: 4.45724\n",
      "[240]\ttraining's rmse: 4.458\n",
      "[250]\ttraining's rmse: 4.45866\n",
      "[260]\ttraining's rmse: 4.45917\n",
      "[270]\ttraining's rmse: 4.46102\n",
      "[280]\ttraining's rmse: 4.4616\n",
      "[290]\ttraining's rmse: 4.463\n",
      "[300]\ttraining's rmse: 4.46383\n",
      "train model; day 16\n",
      "[10]\ttraining's rmse: 4.39699\n",
      "[20]\ttraining's rmse: 4.38064\n",
      "[30]\ttraining's rmse: 4.39364\n",
      "[40]\ttraining's rmse: 4.40797\n",
      "[50]\ttraining's rmse: 4.41731\n",
      "[60]\ttraining's rmse: 4.42344\n",
      "[70]\ttraining's rmse: 4.42963\n",
      "[80]\ttraining's rmse: 4.43269\n",
      "[90]\ttraining's rmse: 4.43606\n",
      "[100]\ttraining's rmse: 4.43884\n",
      "[110]\ttraining's rmse: 4.44006\n",
      "[120]\ttraining's rmse: 4.44178\n",
      "[130]\ttraining's rmse: 4.44446\n",
      "[140]\ttraining's rmse: 4.44645\n",
      "[150]\ttraining's rmse: 4.44743\n",
      "[160]\ttraining's rmse: 4.44944\n",
      "[170]\ttraining's rmse: 4.4505\n",
      "[180]\ttraining's rmse: 4.45184\n",
      "[190]\ttraining's rmse: 4.45381\n",
      "[200]\ttraining's rmse: 4.45545\n",
      "[210]\ttraining's rmse: 4.45649\n",
      "[220]\ttraining's rmse: 4.45807\n",
      "[230]\ttraining's rmse: 4.45897\n",
      "[240]\ttraining's rmse: 4.45972\n",
      "[250]\ttraining's rmse: 4.46067\n",
      "[260]\ttraining's rmse: 4.46143\n",
      "[270]\ttraining's rmse: 4.46256\n",
      "[280]\ttraining's rmse: 4.46295\n",
      "[290]\ttraining's rmse: 4.46363\n",
      "[300]\ttraining's rmse: 4.46513\n",
      "train model; day 17\n",
      "[10]\ttraining's rmse: 4.39689\n",
      "[20]\ttraining's rmse: 4.38099\n",
      "[30]\ttraining's rmse: 4.39319\n",
      "[40]\ttraining's rmse: 4.40679\n",
      "[50]\ttraining's rmse: 4.41616\n",
      "[60]\ttraining's rmse: 4.42325\n",
      "[70]\ttraining's rmse: 4.42819\n",
      "[80]\ttraining's rmse: 4.4315\n",
      "[90]\ttraining's rmse: 4.43541\n",
      "[100]\ttraining's rmse: 4.43825\n",
      "[110]\ttraining's rmse: 4.44055\n",
      "[120]\ttraining's rmse: 4.44252\n",
      "[130]\ttraining's rmse: 4.44436\n",
      "[140]\ttraining's rmse: 4.44693\n",
      "[150]\ttraining's rmse: 4.44788\n",
      "[160]\ttraining's rmse: 4.44952\n",
      "[170]\ttraining's rmse: 4.45074\n",
      "[180]\ttraining's rmse: 4.45106\n",
      "[190]\ttraining's rmse: 4.45306\n",
      "[200]\ttraining's rmse: 4.45483\n",
      "[210]\ttraining's rmse: 4.45546\n",
      "[220]\ttraining's rmse: 4.45694\n",
      "[230]\ttraining's rmse: 4.45767\n",
      "[240]\ttraining's rmse: 4.45842\n",
      "[250]\ttraining's rmse: 4.45888\n",
      "[260]\ttraining's rmse: 4.46062\n",
      "[270]\ttraining's rmse: 4.4612\n",
      "[280]\ttraining's rmse: 4.46118\n",
      "[290]\ttraining's rmse: 4.46267\n",
      "[300]\ttraining's rmse: 4.464\n",
      "train model; day 18\n",
      "[10]\ttraining's rmse: 4.39718\n",
      "[20]\ttraining's rmse: 4.38068\n",
      "[30]\ttraining's rmse: 4.39396\n",
      "[40]\ttraining's rmse: 4.40788\n",
      "[50]\ttraining's rmse: 4.41606\n",
      "[60]\ttraining's rmse: 4.42316\n",
      "[70]\ttraining's rmse: 4.42807\n",
      "[80]\ttraining's rmse: 4.43217\n",
      "[90]\ttraining's rmse: 4.43542\n",
      "[100]\ttraining's rmse: 4.43803\n",
      "[110]\ttraining's rmse: 4.43928\n",
      "[120]\ttraining's rmse: 4.44146\n",
      "[130]\ttraining's rmse: 4.44462\n",
      "[140]\ttraining's rmse: 4.44639\n",
      "[150]\ttraining's rmse: 4.44806\n",
      "[160]\ttraining's rmse: 4.44949\n",
      "[170]\ttraining's rmse: 4.45043\n",
      "[180]\ttraining's rmse: 4.45134\n",
      "[190]\ttraining's rmse: 4.45418\n",
      "[200]\ttraining's rmse: 4.45516\n",
      "[210]\ttraining's rmse: 4.45628\n",
      "[220]\ttraining's rmse: 4.45707\n",
      "[230]\ttraining's rmse: 4.45765\n",
      "[240]\ttraining's rmse: 4.45792\n",
      "[250]\ttraining's rmse: 4.45945\n",
      "[260]\ttraining's rmse: 4.46079\n",
      "[270]\ttraining's rmse: 4.46112\n",
      "[280]\ttraining's rmse: 4.4628\n",
      "[290]\ttraining's rmse: 4.46309\n",
      "[300]\ttraining's rmse: 4.46386\n",
      "train model; day 19\n",
      "[10]\ttraining's rmse: 4.39721\n",
      "[20]\ttraining's rmse: 4.38097\n",
      "[30]\ttraining's rmse: 4.39452\n",
      "[40]\ttraining's rmse: 4.40786\n",
      "[50]\ttraining's rmse: 4.41709\n",
      "[60]\ttraining's rmse: 4.42405\n",
      "[70]\ttraining's rmse: 4.42859\n",
      "[80]\ttraining's rmse: 4.43305\n",
      "[90]\ttraining's rmse: 4.43652\n",
      "[100]\ttraining's rmse: 4.44035\n",
      "[110]\ttraining's rmse: 4.44171\n",
      "[120]\ttraining's rmse: 4.44384\n",
      "[130]\ttraining's rmse: 4.44536\n",
      "[140]\ttraining's rmse: 4.44708\n",
      "[150]\ttraining's rmse: 4.44864\n",
      "[160]\ttraining's rmse: 4.44947\n",
      "[170]\ttraining's rmse: 4.45002\n",
      "[180]\ttraining's rmse: 4.45207\n",
      "[190]\ttraining's rmse: 4.45342\n",
      "[200]\ttraining's rmse: 4.45447\n",
      "[210]\ttraining's rmse: 4.45492\n",
      "[220]\ttraining's rmse: 4.45577\n",
      "[230]\ttraining's rmse: 4.4568\n",
      "[240]\ttraining's rmse: 4.45801\n",
      "[250]\ttraining's rmse: 4.45895\n",
      "[260]\ttraining's rmse: 4.46091\n",
      "[270]\ttraining's rmse: 4.46179\n",
      "[280]\ttraining's rmse: 4.46201\n",
      "[290]\ttraining's rmse: 4.46227\n",
      "[300]\ttraining's rmse: 4.46436\n",
      "train model; day 20\n",
      "[10]\ttraining's rmse: 4.39724\n",
      "[20]\ttraining's rmse: 4.38118\n",
      "[30]\ttraining's rmse: 4.39447\n",
      "[40]\ttraining's rmse: 4.40766\n",
      "[50]\ttraining's rmse: 4.41646\n",
      "[60]\ttraining's rmse: 4.42392\n",
      "[70]\ttraining's rmse: 4.42831\n",
      "[80]\ttraining's rmse: 4.43318\n",
      "[90]\ttraining's rmse: 4.43591\n",
      "[100]\ttraining's rmse: 4.43821\n",
      "[110]\ttraining's rmse: 4.43981\n",
      "[120]\ttraining's rmse: 4.44253\n",
      "[130]\ttraining's rmse: 4.44519\n",
      "[140]\ttraining's rmse: 4.4465\n",
      "[150]\ttraining's rmse: 4.44769\n",
      "[160]\ttraining's rmse: 4.44891\n",
      "[170]\ttraining's rmse: 4.45026\n",
      "[180]\ttraining's rmse: 4.45093\n",
      "[190]\ttraining's rmse: 4.45181\n",
      "[200]\ttraining's rmse: 4.45278\n",
      "[210]\ttraining's rmse: 4.45463\n",
      "[220]\ttraining's rmse: 4.4555\n",
      "[230]\ttraining's rmse: 4.45687\n",
      "[240]\ttraining's rmse: 4.45752\n",
      "[250]\ttraining's rmse: 4.4588\n",
      "[260]\ttraining's rmse: 4.4596\n",
      "[270]\ttraining's rmse: 4.4599\n",
      "[280]\ttraining's rmse: 4.46031\n",
      "[290]\ttraining's rmse: 4.46169\n",
      "[300]\ttraining's rmse: 4.46294\n",
      "train model; day 21\n",
      "[10]\ttraining's rmse: 4.39716\n",
      "[20]\ttraining's rmse: 4.38101\n",
      "[30]\ttraining's rmse: 4.39474\n",
      "[40]\ttraining's rmse: 4.40763\n",
      "[50]\ttraining's rmse: 4.41681\n",
      "[60]\ttraining's rmse: 4.42398\n",
      "[70]\ttraining's rmse: 4.42829\n",
      "[80]\ttraining's rmse: 4.43225\n",
      "[90]\ttraining's rmse: 4.43624\n",
      "[100]\ttraining's rmse: 4.4381\n",
      "[110]\ttraining's rmse: 4.44055\n",
      "[120]\ttraining's rmse: 4.44219\n",
      "[130]\ttraining's rmse: 4.44412\n",
      "[140]\ttraining's rmse: 4.4467\n",
      "[150]\ttraining's rmse: 4.44763\n",
      "[160]\ttraining's rmse: 4.44963\n",
      "[170]\ttraining's rmse: 4.45016\n",
      "[180]\ttraining's rmse: 4.4521\n",
      "[190]\ttraining's rmse: 4.45387\n",
      "[200]\ttraining's rmse: 4.45492\n",
      "[210]\ttraining's rmse: 4.45535\n",
      "[220]\ttraining's rmse: 4.45722\n",
      "[230]\ttraining's rmse: 4.45942\n",
      "[240]\ttraining's rmse: 4.45958\n",
      "[250]\ttraining's rmse: 4.46078\n",
      "[260]\ttraining's rmse: 4.46166\n",
      "[270]\ttraining's rmse: 4.46275\n",
      "[280]\ttraining's rmse: 4.46331\n",
      "[290]\ttraining's rmse: 4.46395\n",
      "[300]\ttraining's rmse: 4.46561\n",
      "train model; day 22\n",
      "[10]\ttraining's rmse: 4.39703\n",
      "[20]\ttraining's rmse: 4.38112\n",
      "[30]\ttraining's rmse: 4.39413\n",
      "[40]\ttraining's rmse: 4.40789\n",
      "[50]\ttraining's rmse: 4.41649\n",
      "[60]\ttraining's rmse: 4.4234\n",
      "[70]\ttraining's rmse: 4.42896\n",
      "[80]\ttraining's rmse: 4.43341\n",
      "[90]\ttraining's rmse: 4.43657\n",
      "[100]\ttraining's rmse: 4.43874\n",
      "[110]\ttraining's rmse: 4.44093\n",
      "[120]\ttraining's rmse: 4.44248\n",
      "[130]\ttraining's rmse: 4.44493\n",
      "[140]\ttraining's rmse: 4.44695\n",
      "[150]\ttraining's rmse: 4.44773\n",
      "[160]\ttraining's rmse: 4.44908\n",
      "[170]\ttraining's rmse: 4.45073\n",
      "[180]\ttraining's rmse: 4.4513\n",
      "[190]\ttraining's rmse: 4.45309\n",
      "[200]\ttraining's rmse: 4.45382\n",
      "[210]\ttraining's rmse: 4.45526\n",
      "[220]\ttraining's rmse: 4.45603\n",
      "[230]\ttraining's rmse: 4.45648\n",
      "[240]\ttraining's rmse: 4.45749\n",
      "[250]\ttraining's rmse: 4.45826\n",
      "[260]\ttraining's rmse: 4.45883\n",
      "[270]\ttraining's rmse: 4.45936\n",
      "[280]\ttraining's rmse: 4.46028\n",
      "[290]\ttraining's rmse: 4.46087\n",
      "[300]\ttraining's rmse: 4.46205\n",
      "train model; day 23\n",
      "[10]\ttraining's rmse: 4.39699\n",
      "[20]\ttraining's rmse: 4.38136\n",
      "[30]\ttraining's rmse: 4.39424\n",
      "[40]\ttraining's rmse: 4.40739\n",
      "[50]\ttraining's rmse: 4.41657\n",
      "[60]\ttraining's rmse: 4.42343\n",
      "[70]\ttraining's rmse: 4.42828\n",
      "[80]\ttraining's rmse: 4.43267\n",
      "[90]\ttraining's rmse: 4.43606\n",
      "[100]\ttraining's rmse: 4.43861\n",
      "[110]\ttraining's rmse: 4.44107\n",
      "[120]\ttraining's rmse: 4.44306\n",
      "[130]\ttraining's rmse: 4.4455\n",
      "[140]\ttraining's rmse: 4.44701\n",
      "[150]\ttraining's rmse: 4.44876\n",
      "[160]\ttraining's rmse: 4.45135\n",
      "[170]\ttraining's rmse: 4.45274\n",
      "[180]\ttraining's rmse: 4.45349\n",
      "[190]\ttraining's rmse: 4.4538\n",
      "[200]\ttraining's rmse: 4.45505\n",
      "[210]\ttraining's rmse: 4.45631\n",
      "[220]\ttraining's rmse: 4.45732\n",
      "[230]\ttraining's rmse: 4.45717\n",
      "[240]\ttraining's rmse: 4.45782\n",
      "[250]\ttraining's rmse: 4.45898\n",
      "[260]\ttraining's rmse: 4.45938\n",
      "[270]\ttraining's rmse: 4.46116\n",
      "[280]\ttraining's rmse: 4.46164\n",
      "[290]\ttraining's rmse: 4.46283\n",
      "[300]\ttraining's rmse: 4.46419\n",
      "train model; day 24\n",
      "[10]\ttraining's rmse: 4.39736\n",
      "[20]\ttraining's rmse: 4.38145\n",
      "[30]\ttraining's rmse: 4.39423\n",
      "[40]\ttraining's rmse: 4.40758\n",
      "[50]\ttraining's rmse: 4.41608\n",
      "[60]\ttraining's rmse: 4.42408\n",
      "[70]\ttraining's rmse: 4.42994\n",
      "[80]\ttraining's rmse: 4.43256\n",
      "[90]\ttraining's rmse: 4.43638\n",
      "[100]\ttraining's rmse: 4.43897\n",
      "[110]\ttraining's rmse: 4.44293\n",
      "[120]\ttraining's rmse: 4.44335\n",
      "[130]\ttraining's rmse: 4.44636\n",
      "[140]\ttraining's rmse: 4.44789\n",
      "[150]\ttraining's rmse: 4.44935\n",
      "[160]\ttraining's rmse: 4.45054\n",
      "[170]\ttraining's rmse: 4.45186\n",
      "[180]\ttraining's rmse: 4.4527\n",
      "[190]\ttraining's rmse: 4.45385\n",
      "[200]\ttraining's rmse: 4.45488\n",
      "[210]\ttraining's rmse: 4.45576\n",
      "[220]\ttraining's rmse: 4.45709\n",
      "[230]\ttraining's rmse: 4.4575\n",
      "[240]\ttraining's rmse: 4.45866\n",
      "[250]\ttraining's rmse: 4.45923\n",
      "[260]\ttraining's rmse: 4.46021\n",
      "[270]\ttraining's rmse: 4.46106\n",
      "[280]\ttraining's rmse: 4.46126\n",
      "[290]\ttraining's rmse: 4.46233\n",
      "[300]\ttraining's rmse: 4.46388\n",
      "train model; day 25\n",
      "[10]\ttraining's rmse: 4.39712\n",
      "[20]\ttraining's rmse: 4.38131\n",
      "[30]\ttraining's rmse: 4.39462\n",
      "[40]\ttraining's rmse: 4.40753\n",
      "[50]\ttraining's rmse: 4.41646\n",
      "[60]\ttraining's rmse: 4.42447\n",
      "[70]\ttraining's rmse: 4.42986\n",
      "[80]\ttraining's rmse: 4.43317\n",
      "[90]\ttraining's rmse: 4.43653\n",
      "[100]\ttraining's rmse: 4.43904\n",
      "[110]\ttraining's rmse: 4.44047\n",
      "[120]\ttraining's rmse: 4.44326\n",
      "[130]\ttraining's rmse: 4.44492\n",
      "[140]\ttraining's rmse: 4.44578\n",
      "[150]\ttraining's rmse: 4.44751\n",
      "[160]\ttraining's rmse: 4.44895\n",
      "[170]\ttraining's rmse: 4.44987\n",
      "[180]\ttraining's rmse: 4.45195\n",
      "[190]\ttraining's rmse: 4.45193\n",
      "[200]\ttraining's rmse: 4.45237\n",
      "[210]\ttraining's rmse: 4.45298\n",
      "[220]\ttraining's rmse: 4.45431\n",
      "[230]\ttraining's rmse: 4.45453\n",
      "[240]\ttraining's rmse: 4.45585\n",
      "[250]\ttraining's rmse: 4.4563\n",
      "[260]\ttraining's rmse: 4.45733\n",
      "[270]\ttraining's rmse: 4.45889\n",
      "[280]\ttraining's rmse: 4.46018\n",
      "[290]\ttraining's rmse: 4.4603\n",
      "[300]\ttraining's rmse: 4.46163\n",
      "train model; day 26\n",
      "[10]\ttraining's rmse: 4.39721\n",
      "[20]\ttraining's rmse: 4.38108\n",
      "[30]\ttraining's rmse: 4.39452\n",
      "[40]\ttraining's rmse: 4.40773\n",
      "[50]\ttraining's rmse: 4.41754\n",
      "[60]\ttraining's rmse: 4.42401\n",
      "[70]\ttraining's rmse: 4.4289\n",
      "[80]\ttraining's rmse: 4.43281\n",
      "[90]\ttraining's rmse: 4.43591\n",
      "[100]\ttraining's rmse: 4.43859\n",
      "[110]\ttraining's rmse: 4.44112\n",
      "[120]\ttraining's rmse: 4.44332\n",
      "[130]\ttraining's rmse: 4.44527\n",
      "[140]\ttraining's rmse: 4.44619\n",
      "[150]\ttraining's rmse: 4.44842\n",
      "[160]\ttraining's rmse: 4.45114\n",
      "[170]\ttraining's rmse: 4.45268\n",
      "[180]\ttraining's rmse: 4.45404\n",
      "[190]\ttraining's rmse: 4.45509\n",
      "[200]\ttraining's rmse: 4.45567\n",
      "[210]\ttraining's rmse: 4.45677\n",
      "[220]\ttraining's rmse: 4.45772\n",
      "[230]\ttraining's rmse: 4.45752\n",
      "[240]\ttraining's rmse: 4.45821\n",
      "[250]\ttraining's rmse: 4.45898\n",
      "[260]\ttraining's rmse: 4.45995\n",
      "[270]\ttraining's rmse: 4.46174\n",
      "[280]\ttraining's rmse: 4.46213\n",
      "[290]\ttraining's rmse: 4.46367\n",
      "[300]\ttraining's rmse: 4.46426\n",
      "train model; day 27\n",
      "[10]\ttraining's rmse: 4.39722\n",
      "[20]\ttraining's rmse: 4.38154\n",
      "[30]\ttraining's rmse: 4.39466\n",
      "[40]\ttraining's rmse: 4.40732\n",
      "[50]\ttraining's rmse: 4.41744\n",
      "[60]\ttraining's rmse: 4.42448\n",
      "[70]\ttraining's rmse: 4.42968\n",
      "[80]\ttraining's rmse: 4.43369\n",
      "[90]\ttraining's rmse: 4.4367\n",
      "[100]\ttraining's rmse: 4.43856\n",
      "[110]\ttraining's rmse: 4.44094\n",
      "[120]\ttraining's rmse: 4.44316\n",
      "[130]\ttraining's rmse: 4.44528\n",
      "[140]\ttraining's rmse: 4.44683\n",
      "[150]\ttraining's rmse: 4.44783\n",
      "[160]\ttraining's rmse: 4.44932\n",
      "[170]\ttraining's rmse: 4.45044\n",
      "[180]\ttraining's rmse: 4.45172\n",
      "[190]\ttraining's rmse: 4.45321\n",
      "[200]\ttraining's rmse: 4.45372\n",
      "[210]\ttraining's rmse: 4.45455\n",
      "[220]\ttraining's rmse: 4.45514\n",
      "[230]\ttraining's rmse: 4.45631\n",
      "[240]\ttraining's rmse: 4.45777\n",
      "[250]\ttraining's rmse: 4.45879\n",
      "[260]\ttraining's rmse: 4.45921\n",
      "[270]\ttraining's rmse: 4.45985\n",
      "[280]\ttraining's rmse: 4.46157\n",
      "[290]\ttraining's rmse: 4.46258\n",
      "[300]\ttraining's rmse: 4.46351\n",
      "train model; day 28\n",
      "[10]\ttraining's rmse: 4.39724\n",
      "[20]\ttraining's rmse: 4.38115\n",
      "[30]\ttraining's rmse: 4.39425\n",
      "[40]\ttraining's rmse: 4.40703\n",
      "[50]\ttraining's rmse: 4.41637\n",
      "[60]\ttraining's rmse: 4.42309\n",
      "[70]\ttraining's rmse: 4.42808\n",
      "[80]\ttraining's rmse: 4.43269\n",
      "[90]\ttraining's rmse: 4.43657\n",
      "[100]\ttraining's rmse: 4.43803\n",
      "[110]\ttraining's rmse: 4.44048\n",
      "[120]\ttraining's rmse: 4.44265\n",
      "[130]\ttraining's rmse: 4.44471\n",
      "[140]\ttraining's rmse: 4.4468\n",
      "[150]\ttraining's rmse: 4.44824\n",
      "[160]\ttraining's rmse: 4.44954\n",
      "[170]\ttraining's rmse: 4.4501\n",
      "[180]\ttraining's rmse: 4.45159\n",
      "[190]\ttraining's rmse: 4.45319\n",
      "[200]\ttraining's rmse: 4.45524\n",
      "[210]\ttraining's rmse: 4.45603\n",
      "[220]\ttraining's rmse: 4.45706\n",
      "[230]\ttraining's rmse: 4.45832\n",
      "[240]\ttraining's rmse: 4.45874\n",
      "[250]\ttraining's rmse: 4.45979\n",
      "[260]\ttraining's rmse: 4.45974\n",
      "[270]\ttraining's rmse: 4.46122\n",
      "[280]\ttraining's rmse: 4.46208\n",
      "[290]\ttraining's rmse: 4.46238\n",
      "[300]\ttraining's rmse: 4.46373\n"
     ]
    }
   ],
   "source": [
    "store_id = 'CA_1'\n",
    "sub_start_date = 1942\n",
    "sub_end_date = 1969\n",
    "print('store_id:',store_id)\n",
    "print('load dataset')\n",
    "df = load_data(store_id)\n",
    "day_mask = (df.d<sub_start_date)\n",
    "train = df[day_mask]\n",
    "train_ids = train.id.unique()\n",
    "NUM_ITEMS = len(train_ids)\n",
    "gc.collect()\n",
    "day_mask = (te2.d>=sub_start_date)&(te2.d<=sub_end_date)\n",
    "all_pred = te2[day_mask][['id','d']]\n",
    "all_pred['sales'] = 0\n",
    "\n",
    "for PREDICT_DAY in range(1,29): \n",
    "#        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n",
    "    remove_lag = []\n",
    "    for i in range(1, PREDICT_DAY):\n",
    "        remove_lag.append('sales_lag_{}'.format(i))\n",
    "    new_features = list(set(feature) - set(remove_lag))\n",
    "    print('train model; day', PREDICT_DAY)\n",
    "#     model = run_lgb(train,val,test_tmp, new_features, loss_func_dict[LOSS_NAME], wrmsse)\n",
    "    model = run_lgb_no_early_stopping(train, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params, num_boost_round = 300)\n",
    "    day_mask = df.d == sub_start_date + PREDICT_DAY - 1\n",
    "    pred1 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=100)\n",
    "    pred2 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=200)\n",
    "    pred3 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=300)\n",
    "    pred = (pred1 + pred2 + pred3)/3.0\n",
    "    pred = link_func_dict[LOSS_NAME](pred)\n",
    "    day_mask2 = all_pred.d == sub_start_date + PREDICT_DAY - 1\n",
    "    all_pred.loc[(all_pred.id.isin(train_ids))&(day_mask2),'sales'] += pred\n",
    "\n",
    "    model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n",
    "    pickle.dump(model, open(model_name, 'wb'))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1db209c6-3495-40a1-99d8-269cbc46b1d3",
    "_uuid": "8078d7ce-3400-4ff7-b05f-7faea1dbfd76"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "ad0a6a8f-9188-4c77-9059-82c3ebf7f8c1",
    "_uuid": "fb8f1b4a-8c81-4d4d-8dcb-90cc0e998a50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46881677</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.928976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46881678</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.243297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46881679</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.568621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46881680</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1942</td>\n",
       "      <td>1.647015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46881681</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>1942</td>\n",
       "      <td>1.076813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735392</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735393</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735394</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735395</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47735396</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853720 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     d     sales\n",
       "46881677  HOBBIES_1_001_CA_1_evaluation  1942  0.928976\n",
       "46881678  HOBBIES_1_002_CA_1_evaluation  1942  0.243297\n",
       "46881679  HOBBIES_1_003_CA_1_evaluation  1942  0.568621\n",
       "46881680  HOBBIES_1_004_CA_1_evaluation  1942  1.647015\n",
       "46881681  HOBBIES_1_005_CA_1_evaluation  1942  1.076813\n",
       "...                                 ...   ...       ...\n",
       "47735392    FOODS_3_823_WI_3_evaluation  1969  0.000000\n",
       "47735393    FOODS_3_824_WI_3_evaluation  1969  0.000000\n",
       "47735394    FOODS_3_825_WI_3_evaluation  1969  0.000000\n",
       "47735395    FOODS_3_826_WI_3_evaluation  1969  0.000000\n",
       "47735396    FOODS_3_827_WI_3_evaluation  1969  0.000000\n",
       "\n",
       "[853720 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "2ebd6877-8f97-4ea2-bf06-a930539df02a",
    "_uuid": "eb023700-e98a-48ae-91a4-8cda446227b0"
   },
   "outputs": [],
   "source": [
    "sub = all_pred[['d','sales','id']].pivot(index='id', columns='d', values='sales').reset_index()\n",
    "sub.columns = ['id'] + ['F'+str(i) for i in range(1,29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "f63e3394-d64c-4fda-b49e-2bcc8245ef1b",
    "_uuid": "139fe940-81a8-4456-b085-13ef062958fa"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')[['id']]\n",
    "submission = submission.merge(sub, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "4c25abf4-11ae-45fa-8267-819374078a26",
    "_uuid": "86957d19-2258-4c52-be0c-604c53d04c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>F17</th>\n",
       "      <th>F18</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30490</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>0.928976</td>\n",
       "      <td>0.855067</td>\n",
       "      <td>0.858215</td>\n",
       "      <td>0.958581</td>\n",
       "      <td>0.983425</td>\n",
       "      <td>1.456178</td>\n",
       "      <td>1.241887</td>\n",
       "      <td>1.033486</td>\n",
       "      <td>0.864826</td>\n",
       "      <td>0.992317</td>\n",
       "      <td>0.954108</td>\n",
       "      <td>1.140729</td>\n",
       "      <td>1.611742</td>\n",
       "      <td>1.403242</td>\n",
       "      <td>0.992086</td>\n",
       "      <td>0.954974</td>\n",
       "      <td>0.979796</td>\n",
       "      <td>1.000425</td>\n",
       "      <td>1.053883</td>\n",
       "      <td>1.434949</td>\n",
       "      <td>1.353875</td>\n",
       "      <td>1.003265</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.942741</td>\n",
       "      <td>1.005463</td>\n",
       "      <td>1.018915</td>\n",
       "      <td>1.341483</td>\n",
       "      <td>1.265092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30491</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.243297</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.220332</td>\n",
       "      <td>0.235680</td>\n",
       "      <td>0.286510</td>\n",
       "      <td>0.284943</td>\n",
       "      <td>0.314075</td>\n",
       "      <td>0.272585</td>\n",
       "      <td>0.238823</td>\n",
       "      <td>0.237823</td>\n",
       "      <td>0.200925</td>\n",
       "      <td>0.256331</td>\n",
       "      <td>0.322594</td>\n",
       "      <td>0.324076</td>\n",
       "      <td>0.237421</td>\n",
       "      <td>0.228359</td>\n",
       "      <td>0.223017</td>\n",
       "      <td>0.237833</td>\n",
       "      <td>0.260256</td>\n",
       "      <td>0.340545</td>\n",
       "      <td>0.331627</td>\n",
       "      <td>0.226688</td>\n",
       "      <td>0.218926</td>\n",
       "      <td>0.216586</td>\n",
       "      <td>0.234714</td>\n",
       "      <td>0.278395</td>\n",
       "      <td>0.360082</td>\n",
       "      <td>0.311839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30492</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.568621</td>\n",
       "      <td>0.575983</td>\n",
       "      <td>0.517844</td>\n",
       "      <td>0.611950</td>\n",
       "      <td>0.800337</td>\n",
       "      <td>0.938517</td>\n",
       "      <td>0.942831</td>\n",
       "      <td>0.699381</td>\n",
       "      <td>0.598386</td>\n",
       "      <td>0.587305</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.833949</td>\n",
       "      <td>0.925855</td>\n",
       "      <td>0.868462</td>\n",
       "      <td>0.528867</td>\n",
       "      <td>0.621183</td>\n",
       "      <td>0.611610</td>\n",
       "      <td>0.642353</td>\n",
       "      <td>0.806870</td>\n",
       "      <td>0.866454</td>\n",
       "      <td>0.996938</td>\n",
       "      <td>0.589925</td>\n",
       "      <td>0.568287</td>\n",
       "      <td>0.512017</td>\n",
       "      <td>0.568197</td>\n",
       "      <td>0.661165</td>\n",
       "      <td>0.847047</td>\n",
       "      <td>0.733053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30493</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.647015</td>\n",
       "      <td>1.321682</td>\n",
       "      <td>1.349216</td>\n",
       "      <td>1.424549</td>\n",
       "      <td>1.658432</td>\n",
       "      <td>2.371626</td>\n",
       "      <td>2.403986</td>\n",
       "      <td>1.967961</td>\n",
       "      <td>1.423854</td>\n",
       "      <td>1.504812</td>\n",
       "      <td>1.457067</td>\n",
       "      <td>1.789346</td>\n",
       "      <td>2.399704</td>\n",
       "      <td>3.128019</td>\n",
       "      <td>1.896030</td>\n",
       "      <td>1.504114</td>\n",
       "      <td>1.617622</td>\n",
       "      <td>1.452808</td>\n",
       "      <td>1.673885</td>\n",
       "      <td>2.202945</td>\n",
       "      <td>2.680686</td>\n",
       "      <td>1.564972</td>\n",
       "      <td>1.363706</td>\n",
       "      <td>1.368492</td>\n",
       "      <td>1.376190</td>\n",
       "      <td>1.595480</td>\n",
       "      <td>2.199602</td>\n",
       "      <td>2.522522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30494</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>1.076813</td>\n",
       "      <td>1.024389</td>\n",
       "      <td>0.972239</td>\n",
       "      <td>1.094655</td>\n",
       "      <td>1.102442</td>\n",
       "      <td>1.334581</td>\n",
       "      <td>1.223801</td>\n",
       "      <td>1.160740</td>\n",
       "      <td>1.031813</td>\n",
       "      <td>1.155773</td>\n",
       "      <td>1.144305</td>\n",
       "      <td>1.230134</td>\n",
       "      <td>1.469180</td>\n",
       "      <td>1.628624</td>\n",
       "      <td>1.171196</td>\n",
       "      <td>1.191477</td>\n",
       "      <td>1.186653</td>\n",
       "      <td>1.218645</td>\n",
       "      <td>1.285661</td>\n",
       "      <td>1.551496</td>\n",
       "      <td>1.780227</td>\n",
       "      <td>1.185725</td>\n",
       "      <td>1.135332</td>\n",
       "      <td>1.153467</td>\n",
       "      <td>1.154078</td>\n",
       "      <td>1.335292</td>\n",
       "      <td>1.677327</td>\n",
       "      <td>1.728695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33534</th>\n",
       "      <td>FOODS_3_823_CA_1_evaluation</td>\n",
       "      <td>1.314703</td>\n",
       "      <td>1.243288</td>\n",
       "      <td>1.304350</td>\n",
       "      <td>1.418034</td>\n",
       "      <td>1.559565</td>\n",
       "      <td>2.077377</td>\n",
       "      <td>1.718231</td>\n",
       "      <td>1.334965</td>\n",
       "      <td>1.208735</td>\n",
       "      <td>1.380798</td>\n",
       "      <td>1.337726</td>\n",
       "      <td>1.741983</td>\n",
       "      <td>1.916374</td>\n",
       "      <td>1.834314</td>\n",
       "      <td>1.419283</td>\n",
       "      <td>1.266771</td>\n",
       "      <td>1.332807</td>\n",
       "      <td>1.276075</td>\n",
       "      <td>1.560009</td>\n",
       "      <td>1.625464</td>\n",
       "      <td>1.937608</td>\n",
       "      <td>1.200195</td>\n",
       "      <td>1.079950</td>\n",
       "      <td>1.192248</td>\n",
       "      <td>1.122019</td>\n",
       "      <td>1.438351</td>\n",
       "      <td>1.673465</td>\n",
       "      <td>1.523811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33535</th>\n",
       "      <td>FOODS_3_824_CA_1_evaluation</td>\n",
       "      <td>0.756642</td>\n",
       "      <td>0.649737</td>\n",
       "      <td>0.665257</td>\n",
       "      <td>0.607720</td>\n",
       "      <td>0.631171</td>\n",
       "      <td>0.771392</td>\n",
       "      <td>0.786618</td>\n",
       "      <td>0.864134</td>\n",
       "      <td>0.748610</td>\n",
       "      <td>0.835157</td>\n",
       "      <td>0.753327</td>\n",
       "      <td>0.861994</td>\n",
       "      <td>1.053418</td>\n",
       "      <td>1.054843</td>\n",
       "      <td>0.852388</td>\n",
       "      <td>0.765398</td>\n",
       "      <td>0.849951</td>\n",
       "      <td>0.742238</td>\n",
       "      <td>0.828232</td>\n",
       "      <td>0.925344</td>\n",
       "      <td>0.957741</td>\n",
       "      <td>1.023548</td>\n",
       "      <td>0.896699</td>\n",
       "      <td>0.742778</td>\n",
       "      <td>0.837690</td>\n",
       "      <td>0.858610</td>\n",
       "      <td>0.996295</td>\n",
       "      <td>1.027103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33536</th>\n",
       "      <td>FOODS_3_825_CA_1_evaluation</td>\n",
       "      <td>1.096512</td>\n",
       "      <td>1.001491</td>\n",
       "      <td>0.920515</td>\n",
       "      <td>0.834639</td>\n",
       "      <td>1.084688</td>\n",
       "      <td>1.265747</td>\n",
       "      <td>1.378901</td>\n",
       "      <td>1.188741</td>\n",
       "      <td>0.918048</td>\n",
       "      <td>1.128404</td>\n",
       "      <td>1.014180</td>\n",
       "      <td>1.290350</td>\n",
       "      <td>1.509996</td>\n",
       "      <td>1.747574</td>\n",
       "      <td>1.186110</td>\n",
       "      <td>1.192605</td>\n",
       "      <td>1.058968</td>\n",
       "      <td>1.064783</td>\n",
       "      <td>1.216990</td>\n",
       "      <td>1.407180</td>\n",
       "      <td>1.602597</td>\n",
       "      <td>1.119487</td>\n",
       "      <td>0.994725</td>\n",
       "      <td>1.000791</td>\n",
       "      <td>0.994133</td>\n",
       "      <td>1.156348</td>\n",
       "      <td>1.443164</td>\n",
       "      <td>1.558372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33537</th>\n",
       "      <td>FOODS_3_826_CA_1_evaluation</td>\n",
       "      <td>1.139708</td>\n",
       "      <td>1.201377</td>\n",
       "      <td>1.001293</td>\n",
       "      <td>1.071307</td>\n",
       "      <td>1.090927</td>\n",
       "      <td>1.482208</td>\n",
       "      <td>1.357218</td>\n",
       "      <td>1.184379</td>\n",
       "      <td>1.066727</td>\n",
       "      <td>1.077860</td>\n",
       "      <td>1.013890</td>\n",
       "      <td>1.253249</td>\n",
       "      <td>1.429636</td>\n",
       "      <td>1.487262</td>\n",
       "      <td>1.067063</td>\n",
       "      <td>1.163626</td>\n",
       "      <td>1.099968</td>\n",
       "      <td>1.042238</td>\n",
       "      <td>1.185292</td>\n",
       "      <td>1.529340</td>\n",
       "      <td>1.471300</td>\n",
       "      <td>0.955012</td>\n",
       "      <td>1.136770</td>\n",
       "      <td>0.972879</td>\n",
       "      <td>1.020333</td>\n",
       "      <td>1.023715</td>\n",
       "      <td>1.333369</td>\n",
       "      <td>1.257332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33538</th>\n",
       "      <td>FOODS_3_827_CA_1_evaluation</td>\n",
       "      <td>4.060876</td>\n",
       "      <td>3.385418</td>\n",
       "      <td>3.794659</td>\n",
       "      <td>4.089809</td>\n",
       "      <td>4.670134</td>\n",
       "      <td>5.058366</td>\n",
       "      <td>5.993794</td>\n",
       "      <td>4.736701</td>\n",
       "      <td>3.355965</td>\n",
       "      <td>3.905958</td>\n",
       "      <td>4.292834</td>\n",
       "      <td>4.885342</td>\n",
       "      <td>5.785605</td>\n",
       "      <td>6.438206</td>\n",
       "      <td>4.602936</td>\n",
       "      <td>3.900600</td>\n",
       "      <td>3.796373</td>\n",
       "      <td>4.650415</td>\n",
       "      <td>4.549510</td>\n",
       "      <td>5.348892</td>\n",
       "      <td>6.734334</td>\n",
       "      <td>4.139366</td>\n",
       "      <td>3.565203</td>\n",
       "      <td>3.656747</td>\n",
       "      <td>4.402037</td>\n",
       "      <td>4.759707</td>\n",
       "      <td>5.208923</td>\n",
       "      <td>6.202514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3049 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "30490  HOBBIES_1_001_CA_1_evaluation  0.928976  0.855067  0.858215  0.958581   \n",
       "30491  HOBBIES_1_002_CA_1_evaluation  0.243297  0.210400  0.220332  0.235680   \n",
       "30492  HOBBIES_1_003_CA_1_evaluation  0.568621  0.575983  0.517844  0.611950   \n",
       "30493  HOBBIES_1_004_CA_1_evaluation  1.647015  1.321682  1.349216  1.424549   \n",
       "30494  HOBBIES_1_005_CA_1_evaluation  1.076813  1.024389  0.972239  1.094655   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "33534    FOODS_3_823_CA_1_evaluation  1.314703  1.243288  1.304350  1.418034   \n",
       "33535    FOODS_3_824_CA_1_evaluation  0.756642  0.649737  0.665257  0.607720   \n",
       "33536    FOODS_3_825_CA_1_evaluation  1.096512  1.001491  0.920515  0.834639   \n",
       "33537    FOODS_3_826_CA_1_evaluation  1.139708  1.201377  1.001293  1.071307   \n",
       "33538    FOODS_3_827_CA_1_evaluation  4.060876  3.385418  3.794659  4.089809   \n",
       "\n",
       "             F5        F6        F7        F8        F9       F10       F11  \\\n",
       "30490  0.983425  1.456178  1.241887  1.033486  0.864826  0.992317  0.954108   \n",
       "30491  0.286510  0.284943  0.314075  0.272585  0.238823  0.237823  0.200925   \n",
       "30492  0.800337  0.938517  0.942831  0.699381  0.598386  0.587305  0.596491   \n",
       "30493  1.658432  2.371626  2.403986  1.967961  1.423854  1.504812  1.457067   \n",
       "30494  1.102442  1.334581  1.223801  1.160740  1.031813  1.155773  1.144305   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33534  1.559565  2.077377  1.718231  1.334965  1.208735  1.380798  1.337726   \n",
       "33535  0.631171  0.771392  0.786618  0.864134  0.748610  0.835157  0.753327   \n",
       "33536  1.084688  1.265747  1.378901  1.188741  0.918048  1.128404  1.014180   \n",
       "33537  1.090927  1.482208  1.357218  1.184379  1.066727  1.077860  1.013890   \n",
       "33538  4.670134  5.058366  5.993794  4.736701  3.355965  3.905958  4.292834   \n",
       "\n",
       "            F12       F13       F14       F15       F16       F17       F18  \\\n",
       "30490  1.140729  1.611742  1.403242  0.992086  0.954974  0.979796  1.000425   \n",
       "30491  0.256331  0.322594  0.324076  0.237421  0.228359  0.223017  0.237833   \n",
       "30492  0.833949  0.925855  0.868462  0.528867  0.621183  0.611610  0.642353   \n",
       "30493  1.789346  2.399704  3.128019  1.896030  1.504114  1.617622  1.452808   \n",
       "30494  1.230134  1.469180  1.628624  1.171196  1.191477  1.186653  1.218645   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33534  1.741983  1.916374  1.834314  1.419283  1.266771  1.332807  1.276075   \n",
       "33535  0.861994  1.053418  1.054843  0.852388  0.765398  0.849951  0.742238   \n",
       "33536  1.290350  1.509996  1.747574  1.186110  1.192605  1.058968  1.064783   \n",
       "33537  1.253249  1.429636  1.487262  1.067063  1.163626  1.099968  1.042238   \n",
       "33538  4.885342  5.785605  6.438206  4.602936  3.900600  3.796373  4.650415   \n",
       "\n",
       "            F19       F20       F21       F22       F23       F24       F25  \\\n",
       "30490  1.053883  1.434949  1.353875  1.003265  0.867500  0.942741  1.005463   \n",
       "30491  0.260256  0.340545  0.331627  0.226688  0.218926  0.216586  0.234714   \n",
       "30492  0.806870  0.866454  0.996938  0.589925  0.568287  0.512017  0.568197   \n",
       "30493  1.673885  2.202945  2.680686  1.564972  1.363706  1.368492  1.376190   \n",
       "30494  1.285661  1.551496  1.780227  1.185725  1.135332  1.153467  1.154078   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33534  1.560009  1.625464  1.937608  1.200195  1.079950  1.192248  1.122019   \n",
       "33535  0.828232  0.925344  0.957741  1.023548  0.896699  0.742778  0.837690   \n",
       "33536  1.216990  1.407180  1.602597  1.119487  0.994725  1.000791  0.994133   \n",
       "33537  1.185292  1.529340  1.471300  0.955012  1.136770  0.972879  1.020333   \n",
       "33538  4.549510  5.348892  6.734334  4.139366  3.565203  3.656747  4.402037   \n",
       "\n",
       "            F26       F27       F28  \n",
       "30490  1.018915  1.341483  1.265092  \n",
       "30491  0.278395  0.360082  0.311839  \n",
       "30492  0.661165  0.847047  0.733053  \n",
       "30493  1.595480  2.199602  2.522522  \n",
       "30494  1.335292  1.677327  1.728695  \n",
       "...         ...       ...       ...  \n",
       "33534  1.438351  1.673465  1.523811  \n",
       "33535  0.858610  0.996295  1.027103  \n",
       "33536  1.156348  1.443164  1.558372  \n",
       "33537  1.023715  1.333369  1.257332  \n",
       "33538  4.759707  5.208923  6.202514  \n",
       "\n",
       "[3049 rows x 29 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission.F1!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8dcc92a-540e-49f3-8f27-21bbb567ad88",
    "_uuid": "39c235f1-65aa-446a-a21c-d899169edb42"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
